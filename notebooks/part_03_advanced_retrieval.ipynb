{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Retrieval (Multi-Query + RAG-Fusion)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand limitations of single-query retrieval\n",
    "2. Generate multiple query perspectives using LLMs\n",
    "3. Implement multi-query retrieval for improved coverage\n",
    "4. Apply Reciprocal Rank Fusion (RRF) to combine results\n",
    "5. Build a RAG-Fusion pipeline\n",
    "6. Compare retrieval strategies for security use cases\n",
    "\n",
    "## The Problem with Basic Retrieval\n",
    "\n",
    "In Part 2, we built a basic RAG system that retrieves documents based on similarity to a single query. However, this has limitations:\n",
    "\n",
    "### Example: \"How do attackers steal ML models?\"\n",
    "\n",
    "**Single query retrieval might miss:**\n",
    "- Documents using different terminology (\"model extraction\" vs \"model theft\")\n",
    "- Related concepts (\"knowledge distillation\", \"API exploitation\")\n",
    "- Different perspectives (attacker techniques vs defender detection)\n",
    "- Broader context (intellectual property protection)\n",
    "\n",
    "### Solution: Multi-Query Retrieval\n",
    "\n",
    "Instead of one query, generate multiple related queries:\n",
    "1. \"What are model extraction techniques?\"\n",
    "2. \"How to detect model theft attempts?\"\n",
    "3. \"What is knowledge distillation in model stealing?\"\n",
    "4. \"How do attackers replicate proprietary models?\"\n",
    "5. \"What defenses prevent unauthorized model access?\"\n",
    "\n",
    "Each query retrieves different relevant documents, improving overall coverage.\n",
    "\n",
    "## Approaches We'll Implement\n",
    "\n",
    "1. **Multi-Query Retrieval**: Generate query variations, retrieve for each, merge unique results\n",
    "2. **RAG-Fusion**: Generate related queries, retrieve for each, apply RRF to rank results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "Let's start by loading our existing vector store from Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import Document\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings and LLM\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embeddings and LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing vector store from Part 2\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"owasp_llm_top10\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../data/chroma_db\"\n",
    ")\n",
    "\n",
    "# Create basic retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store loaded\")\n",
    "print(f\"   Collection: {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Multi-Query Retrieval\n",
    "\n",
    "### Concept\n",
    "\n",
    "Multi-query retrieval generates multiple perspectives of the same question and retrieves documents for each. This helps capture:\n",
    "- **Synonyms**: Different ways to express the same concept\n",
    "- **Perspectives**: Attacker vs defender viewpoints\n",
    "- **Granularity**: High-level vs technical details\n",
    "- **Related Concepts**: Adjacent security topics\n",
    "\n",
    "### Implementation Steps\n",
    "\n",
    "1. **Generate Query Variations**: Use LLM to create 5 related queries\n",
    "2. **Retrieve for Each**: Get top-k documents for each variation\n",
    "3. **Merge Unique**: Combine results, removing duplicates\n",
    "4. **Return Top Results**: Return most relevant unique documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for generating query variations\n",
    "multi_query_template = \"\"\"You are an AI assistant specialized in cybersecurity and LLM security.\n",
    "Your task is to generate 5 different versions of the given security question to help retrieve relevant documents from a security knowledge base.\n",
    "\n",
    "Generate variations that:\n",
    "1. Use different terminology (synonyms, technical terms, common phrases)\n",
    "2. Explore different perspectives (attacker view, defender view, analyst view)\n",
    "3. Vary the level of detail (high-level, technical, specific)\n",
    "4. Include related concepts and adjacent topics\n",
    "5. Cover different aspects of the security concern\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Provide 5 alternative versions of this question, one per line:\"\"\"\n",
    "\n",
    "multi_query_prompt = ChatPromptTemplate.from_template(multi_query_template)\n",
    "\n",
    "# Create chain to generate queries\n",
    "generate_queries_chain = (\n",
    "    multi_query_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-query generator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query generation\n",
    "test_question = \"How do attackers steal ML models?\"\n",
    "print(f\"üîç Original Question: {test_question}\")\n",
    "print(\"\\nüìù Generated Query Variations:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "generated_queries = generate_queries_chain.invoke({\"question\": test_question})\n",
    "for i, query in enumerate(generated_queries, 1):\n",
    "    if query.strip():\n",
    "        print(f\"{i}. {query.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_retrieval(question: str, retriever, k: int = 3) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieve documents using multiple query variations.\n",
    "    \n",
    "    Args:\n",
    "        question: Original question\n",
    "        retriever: Vector store retriever\n",
    "        k: Number of documents to retrieve per query\n",
    "        \n",
    "    Returns:\n",
    "        List of unique documents\n",
    "    \"\"\"\n",
    "    # Generate query variations\n",
    "    queries = generate_queries_chain.invoke({\"question\": question})\n",
    "    queries = [q.strip() for q in queries if q.strip()]\n",
    "    \n",
    "    # Add original question\n",
    "    all_queries = [question] + queries\n",
    "    \n",
    "    print(f\"\\nüîÑ Retrieving for {len(all_queries)} query variations...\")\n",
    "    \n",
    "    # Retrieve documents for each query\n",
    "    all_docs = []\n",
    "    seen_content = set()\n",
    "    \n",
    "    for i, query in enumerate(all_queries):\n",
    "        print(f\"   Query {i+1}: {query[:60]}...\")\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # Add unique documents only\n",
    "        for doc in docs:\n",
    "            content_hash = hash(doc.page_content)\n",
    "            if content_hash not in seen_content:\n",
    "                seen_content.add(content_hash)\n",
    "                all_docs.append(doc)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Retrieved {len(all_docs)} unique documents (from {len(all_queries) * k} total)\")\n",
    "    \n",
    "    return all_docs[:k * 2]  # Return top 2k unique documents\n",
    "\n",
    "print(\"‚úÖ Multi-query retrieval function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-query retrieval\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ Testing Multi-Query Retrieval\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "question = \"How do attackers steal ML models?\"\n",
    "print(f\"\\n‚ùì Question: {question}\\n\")\n",
    "\n",
    "# Compare basic retrieval vs multi-query\n",
    "print(\"\\n1Ô∏è‚É£  BASIC RETRIEVAL (Single Query):\")\n",
    "print(\"-\" * 80)\n",
    "basic_docs = retriever.get_relevant_documents(question)\n",
    "print(f\"Retrieved {len(basic_docs)} documents:\\n\")\n",
    "for i, doc in enumerate(basic_docs, 1):\n",
    "    print(f\"{i}. {doc.metadata['id']}: {doc.metadata['title']}\")\n",
    "    print(f\"   Preview: {doc.page_content[:100]}...\\n\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  MULTI-QUERY RETRIEVAL (Multiple Perspectives):\")\n",
    "print(\"-\" * 80)\n",
    "multi_docs = multi_query_retrieval(question, retriever)\n",
    "print(f\"\\nRetrieved {len(multi_docs)} unique documents:\\n\")\n",
    "for i, doc in enumerate(multi_docs, 1):\n",
    "    print(f\"{i}. {doc.metadata['id']}: {doc.metadata['title']}\")\n",
    "    print(f\"   Preview: {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. RAG-Fusion with Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "### Concept\n",
    "\n",
    "RAG-Fusion improves on multi-query by intelligently ranking combined results using **Reciprocal Rank Fusion (RRF)**.\n",
    "\n",
    "### Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "RRF is a simple but effective algorithm for combining ranked lists:\n",
    "\n",
    "```\n",
    "RRF_score(doc) = Œ£ [ 1 / (k + rank(doc, query_i)) ]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `rank(doc, query_i)` = position of document in results for query i (1-indexed)\n",
    "- `k` = constant (typically 60) to prevent division by very small numbers\n",
    "- Higher score = more relevant across multiple queries\n",
    "\n",
    "### Example\n",
    "\n",
    "**Document A**:\n",
    "- Rank 1 in Query 1: 1/(60+1) = 0.0164\n",
    "- Rank 3 in Query 2: 1/(60+3) = 0.0159\n",
    "- Not in Query 3: 0\n",
    "- **Total RRF: 0.0323**\n",
    "\n",
    "**Document B**:\n",
    "- Rank 2 in Query 1: 1/(60+2) = 0.0161\n",
    "- Rank 2 in Query 2: 1/(60+2) = 0.0161\n",
    "- Rank 5 in Query 3: 1/(60+5) = 0.0154\n",
    "- **Total RRF: 0.0476** ‚Üê Higher score, more consistently relevant\n",
    "\n",
    "### Why RRF Works\n",
    "\n",
    "- **Rank-based**: Uses position, not raw similarity scores (more robust)\n",
    "- **Multiple evidence**: Documents appearing in multiple queries get boosted\n",
    "- **Balanced**: No single query dominates the ranking\n",
    "- **Simple**: No parameters to tune (except k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: List[List[Document]], k: int = 60) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Apply Reciprocal Rank Fusion to combine multiple ranked lists.\n",
    "    \n",
    "    Args:\n",
    "        results: List of ranked document lists (one per query)\n",
    "        k: Constant for RRF formula (default: 60)\n",
    "        \n",
    "    Returns:\n",
    "        List of (document, rrf_score) tuples, sorted by score\n",
    "    \"\"\"\n",
    "    # Dictionary to accumulate RRF scores\n",
    "    rrf_scores = defaultdict(float)\n",
    "    \n",
    "    # For each query's results\n",
    "    for doc_list in results:\n",
    "        # For each document in the ranked list\n",
    "        for rank, doc in enumerate(doc_list, 1):\n",
    "            # Use content hash as unique identifier\n",
    "            doc_id = hash(doc.page_content)\n",
    "            \n",
    "            # Add RRF score: 1 / (k + rank)\n",
    "            rrf_scores[doc_id] += 1.0 / (k + rank)\n",
    "    \n",
    "    # Create list of (document, score) tuples\n",
    "    # Need to map back from hash to actual document\n",
    "    doc_map = {}\n",
    "    for doc_list in results:\n",
    "        for doc in doc_list:\n",
    "            doc_id = hash(doc.page_content)\n",
    "            if doc_id not in doc_map:\n",
    "                doc_map[doc_id] = doc\n",
    "    \n",
    "    # Create scored document list\n",
    "    scored_docs = [(doc_map[doc_id], score) for doc_id, score in rrf_scores.items()]\n",
    "    \n",
    "    # Sort by RRF score (descending)\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scored_docs\n",
    "\n",
    "print(\"‚úÖ RRF function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_fusion_retrieval(question: str, retriever, k: int = 3) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    RAG-Fusion: Generate multiple queries, retrieve, and apply RRF.\n",
    "    \n",
    "    Args:\n",
    "        question: Original question\n",
    "        retriever: Vector store retriever\n",
    "        k: Number of documents to retrieve per query\n",
    "        \n",
    "    Returns:\n",
    "        List of (document, rrf_score) tuples\n",
    "    \"\"\"\n",
    "    # Generate query variations\n",
    "    queries = generate_queries_chain.invoke({\"question\": question})\n",
    "    queries = [q.strip() for q in queries if q.strip()]\n",
    "    \n",
    "    # Add original question\n",
    "    all_queries = [question] + queries\n",
    "    \n",
    "    print(f\"\\nüîÑ RAG-Fusion: Processing {len(all_queries)} queries...\")\n",
    "    \n",
    "    # Retrieve documents for each query\n",
    "    all_results = []\n",
    "    for i, query in enumerate(all_queries):\n",
    "        print(f\"   Query {i+1}: {query[:60]}...\")\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        all_results.append(docs)\n",
    "    \n",
    "    # Apply Reciprocal Rank Fusion\n",
    "    print(f\"\\nüîÄ Applying Reciprocal Rank Fusion...\")\n",
    "    fused_results = reciprocal_rank_fusion(all_results)\n",
    "    \n",
    "    print(f\"‚úÖ RAG-Fusion complete: {len(fused_results)} unique documents ranked\\n\")\n",
    "    \n",
    "    return fused_results\n",
    "\n",
    "print(\"‚úÖ RAG-Fusion function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG-Fusion\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ Testing RAG-Fusion\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "question = \"How do attackers steal ML models?\"\n",
    "print(f\"\\n‚ùì Question: {question}\\n\")\n",
    "\n",
    "# RAG-Fusion retrieval\n",
    "fusion_results = rag_fusion_retrieval(question, retriever)\n",
    "\n",
    "print(\"üìä RAG-Fusion Results (Ranked by RRF Score):\\n\")\n",
    "for i, (doc, score) in enumerate(fusion_results[:5], 1):\n",
    "    print(f\"{i}. {doc.metadata['id']}: {doc.metadata['title']}\")\n",
    "    print(f\"   RRF Score: {score:.4f}\")\n",
    "    print(f\"   Preview: {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Building Complete RAG Chains\n",
    "\n",
    "Now let's create complete RAG chains using each retrieval method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for answer generation\n",
    "rag_template = \"\"\"You are an AI security expert assistant helping users understand LLM vulnerabilities and security best practices.\n",
    "\n",
    "Use the following security documentation context to answer the question. Be specific, accurate, and cite relevant details from the context.\n",
    "\n",
    "Guidelines:\n",
    "1. If the answer is in the context, provide a comprehensive explanation with examples.\n",
    "2. Always mention which OWASP LLM vulnerability (LLM01-LLM10) is relevant.\n",
    "3. Include prevention measures and best practices when applicable.\n",
    "4. If the question cannot be answered from the context, say \"I don't have enough information in the documentation to answer that question accurately.\"\n",
    "5. Be concise but thorough - aim for 3-5 paragraphs.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "print(\"‚úÖ RAG prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format documents for context.\"\"\"\n",
    "    return \"\\n\\n\".join([f\"Document {i+1} ({doc.metadata['id']} - {doc.metadata['title']}):\\n{doc.page_content}\" \n",
    "                        for i, doc in enumerate(docs)])\n",
    "\n",
    "# Helper function to format fusion results\n",
    "def format_fusion_docs(fusion_results):\n",
    "    \"\"\"Format fusion results for context.\"\"\"\n",
    "    docs = [doc for doc, score in fusion_results]\n",
    "    return format_docs(docs)\n",
    "\n",
    "print(\"‚úÖ Formatting helpers created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic RAG Chain (from Part 2)\n",
    "basic_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Basic RAG chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Multi-Query RAG Chain\n",
    "def multi_query_rag_chain_func(question: str) -> str:\n",
    "    \"\"\"RAG chain using multi-query retrieval.\"\"\"\n",
    "    docs = multi_query_retrieval(question, retriever)\n",
    "    context = format_docs(docs[:5])  # Use top 5 documents\n",
    "    \n",
    "    prompt_value = rag_prompt.invoke({\"context\": context, \"question\": question})\n",
    "    response = llm.invoke(prompt_value)\n",
    "    return response.content\n",
    "\n",
    "print(\"‚úÖ Multi-Query RAG chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RAG-Fusion Chain\n",
    "def rag_fusion_chain_func(question: str) -> str:\n",
    "    \"\"\"RAG chain using RAG-Fusion.\"\"\"\n",
    "    fusion_results = rag_fusion_retrieval(question, retriever)\n",
    "    context = format_fusion_docs(fusion_results[:5])  # Use top 5 by RRF score\n",
    "    \n",
    "    prompt_value = rag_prompt.invoke({\"context\": context, \"question\": question})\n",
    "    response = llm.invoke(prompt_value)\n",
    "    return response.content\n",
    "\n",
    "print(\"‚úÖ RAG-Fusion chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comprehensive Comparison\n",
    "\n",
    "Let's compare all three approaches on various security questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(question: str):\n",
    "    \"\"\"\n",
    "    Compare basic, multi-query, and RAG-Fusion retrieval for a given question.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚ùì QUESTION: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Basic Retrieval\n",
    "    print(\"\\n1Ô∏è‚É£  BASIC RETRIEVAL\")\n",
    "    print(\"-\" * 80)\n",
    "    basic_docs = retriever.get_relevant_documents(question)\n",
    "    print(f\"Retrieved {len(basic_docs)} documents:\\n\")\n",
    "    for i, doc in enumerate(basic_docs, 1):\n",
    "        print(f\"  {i}. {doc.metadata['id']}: {doc.metadata['title']}\")\n",
    "    \n",
    "    print(\"\\nü§ñ Answer:\")\n",
    "    basic_answer = basic_rag_chain.invoke(question)\n",
    "    print(basic_answer[:300] + \"...\\n\")\n",
    "    \n",
    "    # 2. Multi-Query Retrieval\n",
    "    print(\"\\n2Ô∏è‚É£  MULTI-QUERY RETRIEVAL\")\n",
    "    print(\"-\" * 80)\n",
    "    multi_answer = multi_query_rag_chain_func(question)\n",
    "    print(\"\\nü§ñ Answer:\")\n",
    "    print(multi_answer[:300] + \"...\\n\")\n",
    "    \n",
    "    # 3. RAG-Fusion\n",
    "    print(\"\\n3Ô∏è‚É£  RAG-FUSION\")\n",
    "    print(\"-\" * 80)\n",
    "    fusion_answer = rag_fusion_chain_func(question)\n",
    "    print(\"\\nü§ñ Answer:\")\n",
    "    print(fusion_answer[:300] + \"...\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Comparison function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 1: Model Theft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_retrieval_methods(\"How do attackers steal ML models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 2: Prompt Injection Defenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_retrieval_methods(\"What defenses exist against prompt injection attacks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 3: Information Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_retrieval_methods(\"How can LLMs accidentally leak sensitive information?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Analysis and Recommendations\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "#### üéØ Basic Retrieval\n",
    "\n",
    "**Best for:**\n",
    "- Specific, well-defined questions\n",
    "- When query matches document terminology exactly\n",
    "- Low-latency requirements (single query)\n",
    "- Cost-sensitive applications\n",
    "\n",
    "**Example:** \"What is LLM01 Prompt Injection?\"\n",
    "\n",
    "**Pros:**\n",
    "- Fastest (single retrieval)\n",
    "- Lowest cost (one embedding)\n",
    "- Simple to implement\n",
    "\n",
    "**Cons:**\n",
    "- May miss relevant documents with different wording\n",
    "- Single perspective only\n",
    "- Lower recall\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Multi-Query Retrieval\n",
    "\n",
    "**Best for:**\n",
    "- Ambiguous or broad questions\n",
    "- When you want comprehensive coverage\n",
    "- Exploratory queries\n",
    "- When terminology varies across documents\n",
    "\n",
    "**Example:** \"How do I secure my ML deployment?\"\n",
    "\n",
    "**Pros:**\n",
    "- Better recall (finds more relevant documents)\n",
    "- Covers multiple perspectives\n",
    "- Handles synonyms and paraphrasing\n",
    "\n",
    "**Cons:**\n",
    "- Higher latency (multiple LLM calls + retrievals)\n",
    "- Higher cost (generating queries + multiple embeddings)\n",
    "- May retrieve duplicates\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ RAG-Fusion\n",
    "\n",
    "**Best for:**\n",
    "- Complex, multi-faceted questions\n",
    "- When ranking quality matters most\n",
    "- Research and analysis tasks\n",
    "- When you need documents consistently relevant across perspectives\n",
    "\n",
    "**Example:** \"What are the most effective defenses against adversarial attacks on LLMs?\"\n",
    "\n",
    "**Pros:**\n",
    "- Best ranking quality (RRF scoring)\n",
    "- Prioritizes documents relevant across multiple queries\n",
    "- Robust to single-query biases\n",
    "- Better precision at top-k\n",
    "\n",
    "**Cons:**\n",
    "- Highest latency (query generation + multiple retrievals + fusion)\n",
    "- Highest cost\n",
    "- Most complex implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n",
    "Let's analyze the trade-offs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_retrieval(question: str):\n",
    "    \"\"\"Benchmark retrieval methods.\"\"\"\n",
    "    print(f\"\\n‚è±Ô∏è  Benchmarking retrieval methods for: '{question}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Basic\n",
    "    start = time.time()\n",
    "    basic_docs = retriever.get_relevant_documents(question)\n",
    "    basic_time = time.time() - start\n",
    "    print(f\"\\n1. Basic Retrieval:\")\n",
    "    print(f\"   Time: {basic_time:.2f}s\")\n",
    "    print(f\"   Documents: {len(basic_docs)}\")\n",
    "    print(f\"   Unique: {len(set([doc.page_content for doc in basic_docs]))}\")\n",
    "    \n",
    "    # Multi-Query\n",
    "    start = time.time()\n",
    "    multi_docs = multi_query_retrieval(question, retriever)\n",
    "    multi_time = time.time() - start\n",
    "    print(f\"\\n2. Multi-Query Retrieval:\")\n",
    "    print(f\"   Time: {multi_time:.2f}s ({multi_time/basic_time:.1f}x slower)\")\n",
    "    print(f\"   Documents: {len(multi_docs)}\")\n",
    "    print(f\"   Unique: {len(set([doc.page_content for doc in multi_docs]))}\")\n",
    "    \n",
    "    # RAG-Fusion\n",
    "    start = time.time()\n",
    "    fusion_results = rag_fusion_retrieval(question, retriever)\n",
    "    fusion_time = time.time() - start\n",
    "    print(f\"\\n3. RAG-Fusion:\")\n",
    "    print(f\"   Time: {fusion_time:.2f}s ({fusion_time/basic_time:.1f}x slower)\")\n",
    "    print(f\"   Documents: {len(fusion_results)}\")\n",
    "    print(f\"   Top-3 RRF scores: {[f'{score:.4f}' for _, score in fusion_results[:3]]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"‚úÖ Benchmark function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "benchmark_retrieval(\"How do attackers steal ML models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary and Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "\n",
    "‚úÖ Three advanced retrieval strategies:\n",
    "1. **Multi-Query Retrieval**: Generate query variations for broader coverage\n",
    "2. **RAG-Fusion**: Apply Reciprocal Rank Fusion for intelligent ranking\n",
    "3. **Comparison Framework**: Evaluate approaches systematically\n",
    "\n",
    "### Core Concepts Learned\n",
    "\n",
    "1. **Query Expansion**: Using LLMs to generate related queries\n",
    "2. **Reciprocal Rank Fusion**: Combining ranked lists effectively\n",
    "3. **Trade-offs**: Latency vs quality vs cost\n",
    "4. **Use Cases**: When to apply each approach\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "**Multi-Query Retrieval:**\n",
    "- ‚Üë Better recall (finds more relevant documents)\n",
    "- ‚Üë Handles terminology variations\n",
    "- ‚Üì Higher latency and cost\n",
    "- ‚úÖ Great for exploratory queries\n",
    "\n",
    "**RAG-Fusion:**\n",
    "- ‚Üë‚Üë Best precision at top-k\n",
    "- ‚Üë‚Üë Intelligent ranking across perspectives\n",
    "- ‚Üì‚Üì Highest latency and cost\n",
    "- ‚úÖ Best for complex, important queries\n",
    "\n",
    "**Production Recommendation:**\n",
    "- Use **Basic** for 80% of simple queries (fast, cheap)\n",
    "- Use **Multi-Query** for ambiguous/exploratory queries (medium cost)\n",
    "- Use **RAG-Fusion** for critical/complex queries (highest quality)\n",
    "- Consider **hybrid routing**: classify query complexity, route to appropriate method\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Part 4**, we'll tackle **Query Decomposition**:\n",
    "- Break complex questions into sub-questions\n",
    "- Answer sequentially or in parallel\n",
    "- Synthesize comprehensive answers\n",
    "- Handle multi-step reasoning\n",
    "\n",
    "Example: \"How do I secure my entire ML pipeline?\" ‚Üí\n",
    "1. \"What are security risks in ML training?\"\n",
    "2. \"How to secure ML model deployment?\"\n",
    "3. \"What are inference-time security considerations?\"\n",
    "4. \"How to monitor ML systems for security?\"\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Practice Exercises\n",
    "\n",
    "1. **Experiment with k**: Try different values for k (constant in RRF)\n",
    "2. **Query Templates**: Design different prompt templates for query generation\n",
    "3. **Weighted Fusion**: Modify RRF to weight queries differently\n",
    "4. **Hybrid Retrieval**: Combine dense embeddings with keyword search (BM25)\n",
    "5. **Query Classification**: Build a classifier to route queries to appropriate method\n",
    "\n",
    "### üìö Further Reading\n",
    "\n",
    "- [RAG-Fusion Paper](https://arxiv.org/abs/2402.03367)\n",
    "- [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)\n",
    "- [LangChain Multi-Query Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever)\n",
    "- [Query Expansion Techniques](https://en.wikipedia.org/wiki/Query_expansion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
