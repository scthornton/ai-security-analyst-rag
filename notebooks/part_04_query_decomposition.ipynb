{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Complex Query Handling (Query Decomposition)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand when and why to decompose complex queries\n",
    "2. Generate sub-questions from complex security questions\n",
    "3. Implement sequential answering with context accumulation\n",
    "4. Implement parallel answering with answer synthesis\n",
    "5. Compare sequential vs parallel approaches\n",
    "6. Handle multi-step reasoning for security scenarios\n",
    "\n",
    "## The Problem with Complex Queries\n",
    "\n",
    "Some security questions are too complex to answer in one retrieval:\n",
    "\n",
    "### Example: \"How do I secure my entire ML pipeline?\"\n",
    "\n",
    "This question spans multiple topics:\n",
    "- **Data security**: Training data protection, poisoning prevention\n",
    "- **Training security**: Secure training infrastructure, access controls\n",
    "- **Model security**: Model theft prevention, integrity verification\n",
    "- **Deployment security**: API security, rate limiting, monitoring\n",
    "- **Inference security**: Input validation, output filtering\n",
    "- **Monitoring**: Logging, anomaly detection, incident response\n",
    "\n",
    "**Single retrieval limitations:**\n",
    "- Retrieved documents may only cover 1-2 aspects\n",
    "- LLM context window gets overwhelmed\n",
    "- Answer becomes superficial or incomplete\n",
    "\n",
    "### Solution: Query Decomposition\n",
    "\n",
    "Break the complex question into focused sub-questions:\n",
    "1. \"What are security risks in ML training data?\"\n",
    "2. \"How to secure ML model storage and access?\"\n",
    "3. \"What are inference-time security considerations?\"\n",
    "4. \"How to monitor ML systems for security threats?\"\n",
    "\n",
    "Then answer each sub-question and synthesize results.\n",
    "\n",
    "## Two Approaches\n",
    "\n",
    "### 1. Sequential Decomposition\n",
    "```\n",
    "Question → Sub-Q1 → Answer1 → Sub-Q2 → Answer2 → ... → Final Answer\n",
    "                      ↓          ↓\n",
    "                    Context accumulates iteratively\n",
    "```\n",
    "- Answer each sub-question in order\n",
    "- Pass previous Q&A pairs as context to next question\n",
    "- Build comprehensive answer iteratively\n",
    "- **Best for**: Dependent questions where later questions need earlier context\n",
    "\n",
    "### 2. Parallel Decomposition\n",
    "```\n",
    "Question → Sub-Q1 → Answer1 ↘\n",
    "        → Sub-Q2 → Answer2 → Synthesis → Final Answer\n",
    "        → Sub-Q3 → Answer3 ↗\n",
    "        (all concurrent)\n",
    "```\n",
    "- Answer all sub-questions independently\n",
    "- Faster (parallel processing)\n",
    "- Synthesize all answers into final response\n",
    "- **Best for**: Independent questions that can be answered separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import asyncio\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️  WARNING: OPENAI_API_KEY not found\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings and LLM\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"✅ Embeddings and LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"owasp_llm_top10\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../data/chroma_db\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "print(\"✅ Vector store loaded\")\n",
    "print(f\"   Collection: {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Query Decomposition\n",
    "\n",
    "First, we need to break down complex questions into manageable sub-questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for decomposing questions\n",
    "decomposition_template = \"\"\"You are an AI security expert assistant.\n",
    "Your task is to break down a complex security question into 3-5 focused sub-questions that together fully address the original question.\n",
    "\n",
    "Guidelines:\n",
    "1. Each sub-question should be specific and answerable independently\n",
    "2. Sub-questions should cover different aspects of the main question\n",
    "3. Order sub-questions logically (e.g., from broad to specific, or chronologically)\n",
    "4. Focus on actionable, practical security topics\n",
    "5. Ensure sub-questions are relevant to LLM/ML security when applicable\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Provide 3-5 sub-questions, one per line, numbered:\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(decomposition_template)\n",
    "\n",
    "# Create decomposition chain\n",
    "decompose_chain = (\n",
    "    decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [line.strip() for line in x.split('\\n') if line.strip() and any(c.isdigit() for c in line[:3])])\n",
    ")\n",
    "\n",
    "print(\"✅ Query decomposition chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test decomposition\n",
    "test_question = \"How do I secure my ML deployment pipeline?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"❓ Complex Question: {test_question}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sub_questions = decompose_chain.invoke({\"question\": test_question})\n",
    "\n",
    "print(\"\\n📋 Sub-Questions Generated:\\n\")\n",
    "for i, sq in enumerate(sub_questions, 1):\n",
    "    # Remove number prefix if present\n",
    "    clean_sq = sq.lstrip('0123456789.').strip()\n",
    "    print(f\"{i}. {clean_sq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Sequential Decomposition\n",
    "\n",
    "Answer sub-questions one at a time, passing previous Q&A pairs as context.\n",
    "\n",
    "### Benefits:\n",
    "- **Context accumulation**: Later answers can reference earlier ones\n",
    "- **Coherent narrative**: Builds a logical flow\n",
    "- **Dependency handling**: Later questions can depend on earlier answers\n",
    "\n",
    "### Drawbacks:\n",
    "- **Sequential latency**: Must wait for each answer before proceeding\n",
    "- **Error propagation**: Mistakes in early answers affect later ones\n",
    "- **Token usage**: Context grows with each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for answering sub-questions (sequential)\n",
    "sequential_answer_template = \"\"\"You are an AI security expert assistant.\n",
    "\n",
    "You are answering a series of sub-questions to address a complex security question.\n",
    "\n",
    "Original question: {original_question}\n",
    "\n",
    "Previous Q&A pairs (if any):\n",
    "{previous_qa}\n",
    "\n",
    "Current sub-question: {sub_question}\n",
    "\n",
    "Retrieved context for this sub-question:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Answer the current sub-question using the retrieved context\n",
    "2. Reference previous answers if relevant\n",
    "3. Be specific and cite security best practices\n",
    "4. Keep your answer focused (2-3 paragraphs)\n",
    "5. Mention relevant OWASP vulnerabilities if applicable\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "sequential_answer_prompt = ChatPromptTemplate.from_template(sequential_answer_template)\n",
    "\n",
    "print(\"✅ Sequential answering prompt created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents for context.\"\"\"\n",
    "    return \"\\n\\n\".join([f\"Document {i+1} ({doc.metadata['id']} - {doc.metadata['title']}):\\n{doc.page_content}\" \n",
    "                        for i, doc in enumerate(docs)])\n",
    "\n",
    "def format_previous_qa(qa_pairs: List[tuple]) -> str:\n",
    "    \"\"\"Format previous Q&A pairs.\"\"\"\n",
    "    if not qa_pairs:\n",
    "        return \"(No previous Q&A pairs yet)\"\n",
    "    \n",
    "    formatted = []\n",
    "    for i, (q, a) in enumerate(qa_pairs, 1):\n",
    "        formatted.append(f\"Q{i}: {q}\\nA{i}: {a}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "print(\"✅ Formatting helpers created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_decomposition(question: str, retriever, llm) -> str:\n",
    "    \"\"\"\n",
    "    Answer a complex question using sequential decomposition.\n",
    "    \n",
    "    Args:\n",
    "        question: Complex question to answer\n",
    "        retriever: Vector store retriever\n",
    "        llm: Language model\n",
    "        \n",
    "    Returns:\n",
    "        Final synthesized answer\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔄 Sequential Decomposition: Processing...\\n\")\n",
    "    \n",
    "    # Step 1: Decompose question\n",
    "    print(\"1️⃣  Decomposing question...\")\n",
    "    sub_questions = decompose_chain.invoke({\"question\": question})\n",
    "    sub_questions = [sq.lstrip('0123456789.').strip() for sq in sub_questions]\n",
    "    print(f\"   Generated {len(sub_questions)} sub-questions\\n\")\n",
    "    \n",
    "    # Step 2: Answer sub-questions sequentially\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for i, sub_q in enumerate(sub_questions, 1):\n",
    "        print(f\"2️⃣  Answering sub-question {i}/{len(sub_questions)}...\")\n",
    "        print(f\"   Q: {sub_q}\")\n",
    "        \n",
    "        # Retrieve context for this sub-question\n",
    "        docs = retriever.get_relevant_documents(sub_q)\n",
    "        context = format_docs(docs)\n",
    "        \n",
    "        # Format previous Q&A pairs\n",
    "        previous_qa = format_previous_qa(qa_pairs)\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt_value = sequential_answer_prompt.invoke({\n",
    "            \"original_question\": question,\n",
    "            \"previous_qa\": previous_qa,\n",
    "            \"sub_question\": sub_q,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        response = llm.invoke(prompt_value)\n",
    "        answer = response.content\n",
    "        \n",
    "        # Store Q&A pair\n",
    "        qa_pairs.append((sub_q, answer))\n",
    "        print(f\"   ✓ Answered (context accumulated)\\n\")\n",
    "    \n",
    "    # Step 3: Synthesize final answer\n",
    "    print(\"3️⃣  Synthesizing final answer...\\n\")\n",
    "    \n",
    "    synthesis_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are synthesizing a comprehensive answer to a complex security question.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Sub-questions and answers:\n",
    "{qa_pairs}\n",
    "\n",
    "Instructions:\n",
    "1. Create a cohesive, comprehensive answer to the original question\n",
    "2. Integrate insights from all sub-answers\n",
    "3. Maintain a logical flow and structure\n",
    "4. Highlight key security recommendations\n",
    "5. Keep the answer comprehensive but concise (5-7 paragraphs)\n",
    "\n",
    "Comprehensive Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    qa_text = format_previous_qa(qa_pairs)\n",
    "    final_prompt = synthesis_prompt.invoke({\"question\": question, \"qa_pairs\": qa_text})\n",
    "    final_response = llm.invoke(final_prompt)\n",
    "    \n",
    "    print(\"✅ Sequential decomposition complete!\\n\")\n",
    "    return final_response.content\n",
    "\n",
    "print(\"✅ Sequential decomposition function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sequential decomposition\n",
    "print(\"=\" * 80)\n",
    "print(\"🧪 Testing Sequential Decomposition\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "question = \"How do I secure my ML deployment pipeline?\"\n",
    "print(f\"\\n❓ Question: {question}\\n\")\n",
    "\n",
    "answer = sequential_decomposition(question, retriever, llm)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📄 FINAL ANSWER (Sequential Approach)\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Parallel Decomposition\n",
    "\n",
    "Answer all sub-questions independently, then synthesize.\n",
    "\n",
    "### Benefits:\n",
    "- **Faster**: All sub-questions answered concurrently\n",
    "- **Independent**: No error propagation between sub-answers\n",
    "- **Simpler**: Each sub-question answered in isolation\n",
    "\n",
    "### Drawbacks:\n",
    "- **No context sharing**: Sub-answers can't reference each other\n",
    "- **Synthesis complexity**: Must intelligently combine independent answers\n",
    "- **Potential redundancy**: Sub-answers may overlap without coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for answering sub-questions (parallel)\n",
    "parallel_answer_template = \"\"\"You are an AI security expert assistant.\n",
    "\n",
    "You are answering ONE sub-question that is part of a larger complex question.\n",
    "\n",
    "Original question: {original_question}\n",
    "\n",
    "Sub-question to answer: {sub_question}\n",
    "\n",
    "Retrieved context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Answer this specific sub-question using the retrieved context\n",
    "2. Be comprehensive but focused on this aspect\n",
    "3. Cite security best practices and specific recommendations\n",
    "4. Mention relevant OWASP vulnerabilities if applicable\n",
    "5. Keep your answer self-contained (2-3 paragraphs)\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "parallel_answer_prompt = ChatPromptTemplate.from_template(parallel_answer_template)\n",
    "\n",
    "print(\"✅ Parallel answering prompt created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_sub_question(sub_question: str, original_question: str, retriever, llm) -> tuple:\n",
    "    \"\"\"\n",
    "    Answer a single sub-question.\n",
    "    \n",
    "    Args:\n",
    "        sub_question: The sub-question to answer\n",
    "        original_question: The original complex question\n",
    "        retriever: Vector store retriever\n",
    "        llm: Language model\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (sub_question, answer)\n",
    "    \"\"\"\n",
    "    # Retrieve context\n",
    "    docs = retriever.get_relevant_documents(sub_question)\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt_value = parallel_answer_prompt.invoke({\n",
    "        \"original_question\": original_question,\n",
    "        \"sub_question\": sub_question,\n",
    "        \"context\": context\n",
    "    })\n",
    "    \n",
    "    response = llm.invoke(prompt_value)\n",
    "    return (sub_question, response.content)\n",
    "\n",
    "print(\"✅ Sub-question answering function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_decomposition(question: str, retriever, llm) -> str:\n",
    "    \"\"\"\n",
    "    Answer a complex question using parallel decomposition.\n",
    "    \n",
    "    Args:\n",
    "        question: Complex question to answer\n",
    "        retriever: Vector store retriever\n",
    "        llm: Language model\n",
    "        \n",
    "    Returns:\n",
    "        Final synthesized answer\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔄 Parallel Decomposition: Processing...\\n\")\n",
    "    \n",
    "    # Step 1: Decompose question\n",
    "    print(\"1️⃣  Decomposing question...\")\n",
    "    sub_questions = decompose_chain.invoke({\"question\": question})\n",
    "    sub_questions = [sq.lstrip('0123456789.').strip() for sq in sub_questions]\n",
    "    print(f\"   Generated {len(sub_questions)} sub-questions\\n\")\n",
    "    \n",
    "    # Step 2: Answer all sub-questions in parallel\n",
    "    print(f\"2️⃣  Answering {len(sub_questions)} sub-questions in parallel...\")\n",
    "    \n",
    "    # For demonstration, we'll use sequential execution\n",
    "    # In production, you could use asyncio for true parallelism\n",
    "    qa_pairs = []\n",
    "    for i, sub_q in enumerate(sub_questions, 1):\n",
    "        print(f\"   Answering {i}/{len(sub_questions)}: {sub_q[:50]}...\")\n",
    "        qa_pair = answer_sub_question(sub_q, question, retriever, llm)\n",
    "        qa_pairs.append(qa_pair)\n",
    "    \n",
    "    print(f\"   ✓ All {len(qa_pairs)} sub-questions answered\\n\")\n",
    "    \n",
    "    # Step 3: Synthesize final answer\n",
    "    print(\"3️⃣  Synthesizing final answer from all sub-answers...\\n\")\n",
    "    \n",
    "    synthesis_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are synthesizing a comprehensive answer to a complex security question.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Sub-questions and their independent answers:\n",
    "{qa_pairs}\n",
    "\n",
    "Instructions:\n",
    "1. Create a cohesive, comprehensive answer to the original question\n",
    "2. Integrate insights from all sub-answers into a logical flow\n",
    "3. Remove redundancy while preserving key information\n",
    "4. Organize by themes or stages (e.g., prevention, detection, response)\n",
    "5. Highlight the most critical security recommendations\n",
    "6. Keep the answer comprehensive but well-structured (5-7 paragraphs)\n",
    "\n",
    "Comprehensive Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    qa_text = format_previous_qa(qa_pairs)\n",
    "    final_prompt = synthesis_prompt.invoke({\"question\": question, \"qa_pairs\": qa_text})\n",
    "    final_response = llm.invoke(final_prompt)\n",
    "    \n",
    "    print(\"✅ Parallel decomposition complete!\\n\")\n",
    "    return final_response.content\n",
    "\n",
    "print(\"✅ Parallel decomposition function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parallel decomposition\n",
    "print(\"=\" * 80)\n",
    "print(\"🧪 Testing Parallel Decomposition\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "question = \"How do I secure my ML deployment pipeline?\"\n",
    "print(f\"\\n❓ Question: {question}\\n\")\n",
    "\n",
    "answer = parallel_decomposition(question, retriever, llm)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📄 FINAL ANSWER (Parallel Approach)\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comprehensive Comparison\n",
    "\n",
    "Let's compare both approaches on different types of complex security questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compare_decomposition_methods(question: str):\n",
    "    \"\"\"\n",
    "    Compare sequential vs parallel decomposition.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"❓ COMPLEX QUESTION: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Sequential\n",
    "    print(\"\\n1️⃣  SEQUENTIAL DECOMPOSITION\")\n",
    "    print(\"-\" * 80)\n",
    "    start = time.time()\n",
    "    seq_answer = sequential_decomposition(question, retriever, llm)\n",
    "    seq_time = time.time() - start\n",
    "    \n",
    "    print(f\"\\n⏱️  Time: {seq_time:.2f}s\")\n",
    "    print(f\"\\n📄 Answer Preview:\\n{seq_answer[:300]}...\\n\")\n",
    "    \n",
    "    # Parallel\n",
    "    print(\"\\n2️⃣  PARALLEL DECOMPOSITION\")\n",
    "    print(\"-\" * 80)\n",
    "    start = time.time()\n",
    "    par_answer = parallel_decomposition(question, retriever, llm)\n",
    "    par_time = time.time() - start\n",
    "    \n",
    "    print(f\"\\n⏱️  Time: {par_time:.2f}s (would be faster with true async)\")\n",
    "    print(f\"\\n📄 Answer Preview:\\n{par_answer[:300]}...\\n\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📊 COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Sequential Time: {seq_time:.2f}s\")\n",
    "    print(f\"Parallel Time:   {par_time:.2f}s\")\n",
    "    print(f\"\\nSequential Answer Length: {len(seq_answer)} characters\")\n",
    "    print(f\"Parallel Answer Length:   {len(par_answer)} characters\")\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"✅ Comparison function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 1: End-to-End ML Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_decomposition_methods(\n",
    "    \"What are the comprehensive security measures for deploying an LLM-based application in production?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 2: Attack Surface Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_decomposition_methods(\n",
    "    \"What are all the ways an attacker can compromise an ML system, and how do I defend against each?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 3: Incident Response Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_decomposition_methods(\n",
    "    \"If I suspect my LLM has been compromised, what steps should I take to investigate and respond?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Analysis and Recommendations\n",
    "\n",
    "### When to Use Sequential Decomposition\n",
    "\n",
    "**Best for:**\n",
    "- Questions with **dependent sub-topics** (later parts need earlier context)\n",
    "- **Narrative-style answers** (story-like flow)\n",
    "- **Step-by-step procedures** (each step builds on previous)\n",
    "- **Smaller sub-question count** (3-4 sub-questions)\n",
    "\n",
    "**Examples:**\n",
    "- \"Walk me through securing an ML pipeline from data collection to deployment\"\n",
    "- \"What's the process for investigating a suspected model extraction attack?\"\n",
    "- \"How do I implement defense-in-depth for my LLM application?\"\n",
    "\n",
    "**Pros:**\n",
    "- ✅ Coherent narrative flow\n",
    "- ✅ Sub-answers can reference each other\n",
    "- ✅ Better for procedural knowledge\n",
    "- ✅ More natural synthesis\n",
    "\n",
    "**Cons:**\n",
    "- ❌ Slower (sequential execution)\n",
    "- ❌ Growing context (higher token costs)\n",
    "- ❌ Error propagation risk\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Parallel Decomposition\n",
    "\n",
    "**Best for:**\n",
    "- Questions with **independent sub-topics** (can be answered separately)\n",
    "- **Large sub-question count** (5+ sub-questions)\n",
    "- **Time-sensitive scenarios** (need faster responses)\n",
    "- **Comprehensive coverage** (breadth over narrative)\n",
    "\n",
    "**Examples:**\n",
    "- \"What are all the OWASP Top 10 LLM vulnerabilities and their mitigations?\"\n",
    "- \"Compare different authentication methods for ML APIs\"\n",
    "- \"What security controls should I implement across my ML stack?\"\n",
    "\n",
    "**Pros:**\n",
    "- ✅ Faster (parallel execution possible)\n",
    "- ✅ No error propagation\n",
    "- ✅ Simpler sub-question handling\n",
    "- ✅ Better for comprehensive coverage\n",
    "\n",
    "**Cons:**\n",
    "- ❌ No context sharing between sub-answers\n",
    "- ❌ Potential redundancy in answers\n",
    "- ❌ Synthesis can be challenging\n",
    "\n",
    "---\n",
    "\n",
    "### Hybrid Approach\n",
    "\n",
    "For production systems, consider a **hybrid strategy**:\n",
    "\n",
    "1. **Classify query complexity** (simple, moderate, complex)\n",
    "2. **Identify dependencies** (are sub-questions dependent?)\n",
    "3. **Route appropriately**:\n",
    "   - Simple → Basic RAG (no decomposition)\n",
    "   - Moderate + Independent → Parallel decomposition\n",
    "   - Complex + Dependent → Sequential decomposition\n",
    "4. **Optimize for use case** (speed vs coherence)\n",
    "\n",
    "### Cost Analysis\n",
    "\n",
    "**Sequential Decomposition:**\n",
    "- Decomposition: 1 LLM call\n",
    "- Sub-answers: N LLM calls (with growing context)\n",
    "- Synthesis: 1 LLM call\n",
    "- **Total**: N + 2 LLM calls, higher tokens per call\n",
    "\n",
    "**Parallel Decomposition:**\n",
    "- Decomposition: 1 LLM call\n",
    "- Sub-answers: N LLM calls (independent, smaller context)\n",
    "- Synthesis: 1 LLM call\n",
    "- **Total**: N + 2 LLM calls, lower tokens per call\n",
    "\n",
    "**Conclusion**: Parallel is typically cheaper due to smaller context per call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary and Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "\n",
    "✅ Two query decomposition strategies:\n",
    "1. **Sequential Decomposition**: Iterative answering with context accumulation\n",
    "2. **Parallel Decomposition**: Independent answering with synthesis\n",
    "3. **Comparison Framework**: Systematic evaluation of both approaches\n",
    "\n",
    "### Core Concepts Learned\n",
    "\n",
    "1. **Query Decomposition**: Breaking complex questions into sub-questions\n",
    "2. **Context Management**: Passing Q&A pairs as context (sequential)\n",
    "3. **Independent Processing**: Answering without context sharing (parallel)\n",
    "4. **Answer Synthesis**: Combining sub-answers into cohesive response\n",
    "5. **Trade-offs**: Coherence vs speed, dependencies vs independence\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "**Decomposition Benefits:**\n",
    "- ↑↑ Handles complex, multi-faceted questions\n",
    "- ↑ More comprehensive answers\n",
    "- ↑ Better coverage of all aspects\n",
    "- ✅ Essential for enterprise security analysis\n",
    "\n",
    "**Sequential vs Parallel:**\n",
    "- **Sequential**: Better coherence, slower, dependent sub-questions\n",
    "- **Parallel**: Faster, independent, better for comprehensive coverage\n",
    "- **Hybrid**: Route based on question characteristics\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "1. **Start with classification**: Determine if decomposition is needed\n",
    "2. **Use parallel by default**: Faster and cheaper for most cases\n",
    "3. **Use sequential for procedures**: When order and dependencies matter\n",
    "4. **Implement async for parallel**: True parallelism for speed gains\n",
    "5. **Cache sub-answers**: Reuse answers for similar questions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Part 5**, we'll add **Metadata Filtering**:\n",
    "- Convert natural language to structured queries\n",
    "- Filter by severity, date, affected systems\n",
    "- Use LLM function calling for metadata extraction\n",
    "- Build production-ready filtered retrieval\n",
    "\n",
    "Example: \"Show me critical CVEs affecting PyTorch from the last 6 months\" →\n",
    "```python\n",
    "{\n",
    "  \"severity\": \"Critical\",\n",
    "  \"product\": \"PyTorch\",\n",
    "  \"date_range\": {\"start\": \"2024-04-20\", \"end\": \"2024-10-20\"}\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Practice Exercises\n",
    "\n",
    "1. **Implement True Async**: Use `asyncio` for parallel decomposition\n",
    "2. **Add Validation**: Check if sub-answers actually address sub-questions\n",
    "3. **Optimize Synthesis**: Experiment with different synthesis prompts\n",
    "4. **Add Confidence Scoring**: Rate answer quality for each sub-question\n",
    "5. **Build Query Router**: Automatically choose sequential vs parallel\n",
    "\n",
    "### 📚 Further Reading\n",
    "\n",
    "- [LangChain Query Decomposition](https://python.langchain.com/docs/use_cases/query_decomposition)\n",
    "- [Least-to-Most Prompting](https://arxiv.org/abs/2205.10625)\n",
    "- [Chain-of-Thought Reasoning](https://arxiv.org/abs/2201.11903)\n",
    "- [Tree of Thoughts](https://arxiv.org/abs/2305.10601)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
