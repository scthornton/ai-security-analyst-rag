{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Intelligent Ranking (Re-ranking by Severity)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand why reranking matters for security applications\n",
    "2. Review and apply Reciprocal Rank Fusion (RRF)\n",
    "3. Implement semantic reranking with cross-encoders\n",
    "4. Build security-specific ranking functions\n",
    "5. Combine multiple ranking signals (semantic + metadata)\n",
    "6. Compare different reranking strategies\n",
    "7. Optimize ranking for production use cases\n",
    "\n",
    "## The Problem with Similarity-Only Ranking\n",
    "\n",
    "Vector similarity search ranks documents purely by semantic similarity. For security applications, this has limitations:\n",
    "\n",
    "### Example: \"What are the most critical risks to my ML system?\"\n",
    "\n",
    "**Similarity-only ranking might return:**\n",
    "1. Medium severity vulnerability (high semantic match)\n",
    "2. Low severity vulnerability (mentions \"critical\" in text)\n",
    "3. Critical vulnerability (lower semantic match)\n",
    "\n",
    "**But security practitioners need:**\n",
    "1. **Critical** vulnerability (CVSS 9.0+) with active exploits\n",
    "2. **High** severity vulnerability (CVSS 7.0+) from this year\n",
    "3. **High** severity vulnerability with proof-of-concept\n",
    "\n",
    "## Solution: Intelligent Re-ranking\n",
    "\n",
    "Combine multiple signals to rank documents by both **relevance** and **priority**:\n",
    "\n",
    "1. **Semantic Relevance**: How well does the content match the query?\n",
    "2. **Severity/Risk**: How critical is the vulnerability?\n",
    "3. **Recency**: How recent is the vulnerability?\n",
    "4. **Exploit Status**: Is it being actively exploited?\n",
    "5. **Business Impact**: Does it affect our specific systems?\n",
    "\n",
    "## Reranking Approaches\n",
    "\n",
    "### 1. Reciprocal Rank Fusion (RRF)\n",
    "- Combine multiple ranked lists\n",
    "- Simple, effective, no training needed\n",
    "- Review from Part 3\n",
    "\n",
    "### 2. Cross-Encoder Reranking\n",
    "- Use powerful model to rerank results\n",
    "- More accurate than bi-encoders (embeddings)\n",
    "- Slower but higher quality\n",
    "\n",
    "### 3. Custom Scoring Functions\n",
    "- Weighted combination of signals\n",
    "- Domain-specific (security)\n",
    "- Configurable and explainable\n",
    "\n",
    "### 4. Hybrid Ranking\n",
    "- Combine semantic + metadata scoring\n",
    "- Best of both worlds\n",
    "- Production-ready approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️  WARNING: OPENAI_API_KEY not found\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings and LLM\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"✅ Embeddings and LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"owasp_llm_top10\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../data/chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"✅ Vector store loaded\")\n",
    "print(f\"   Collection: {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Review: Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "We covered RRF in Part 3. Let's review and apply it to security ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(\n",
    "    results: List[List[Document]], \n",
    "    k: int = 60\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Apply Reciprocal Rank Fusion to combine multiple ranked lists.\n",
    "    \n",
    "    RRF Score = Σ [ 1 / (k + rank) ] for each list\n",
    "    \n",
    "    Args:\n",
    "        results: List of ranked document lists\n",
    "        k: Constant for RRF formula (default: 60)\n",
    "        \n",
    "    Returns:\n",
    "        List of (document, rrf_score) tuples, sorted by score\n",
    "    \"\"\"\n",
    "    rrf_scores = defaultdict(float)\n",
    "    doc_map = {}\n",
    "    \n",
    "    # Accumulate RRF scores\n",
    "    for doc_list in results:\n",
    "        for rank, doc in enumerate(doc_list, 1):\n",
    "            doc_id = hash(doc.page_content)\n",
    "            rrf_scores[doc_id] += 1.0 / (k + rank)\n",
    "            if doc_id not in doc_map:\n",
    "                doc_map[doc_id] = doc\n",
    "    \n",
    "    # Create scored list\n",
    "    scored_docs = [(doc_map[doc_id], score) for doc_id, score in rrf_scores.items()]\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scored_docs\n",
    "\n",
    "print(\"✅ RRF function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cross-Encoder Reranking (Semantic)\n",
    "\n",
    "Cross-encoders jointly encode query + document for more accurate relevance scoring.\n",
    "\n",
    "### Bi-Encoder vs Cross-Encoder\n",
    "\n",
    "**Bi-Encoder (Embeddings - what we've been using):**\n",
    "```\n",
    "query → embedding → [0.23, -0.45, ...]\n",
    "doc   → embedding → [0.25, -0.43, ...]\n",
    "similarity = cosine(query_emb, doc_emb)\n",
    "```\n",
    "- Fast: Pre-compute document embeddings\n",
    "- Scalable: Millions of documents\n",
    "- Less accurate: No query-document interaction\n",
    "\n",
    "**Cross-Encoder (Reranking):**\n",
    "```\n",
    "[query, doc] → model → relevance_score\n",
    "```\n",
    "- Slower: Must encode each query-doc pair\n",
    "- Not scalable: Can't pre-compute\n",
    "- More accurate: Models query-document interactions\n",
    "\n",
    "### Strategy: Two-Stage Retrieval\n",
    "\n",
    "1. **Stage 1 (Bi-Encoder)**: Retrieve top 20-50 candidates quickly\n",
    "2. **Stage 2 (Cross-Encoder)**: Rerank top candidates accurately\n",
    "\n",
    "For this demo, we'll implement a simple cross-encoder-style reranking using GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_rerank(\n",
    "    query: str,\n",
    "    documents: List[Document],\n",
    "    llm,\n",
    "    top_k: int = 5\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Rerank documents using LLM to score relevance.\n",
    "    \n",
    "    Note: This is a simplified approach. In production, use:\n",
    "    - Cohere Rerank API\n",
    "    - sentence-transformers cross-encoders\n",
    "    - Specialized reranking models\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        documents: List of candidate documents\n",
    "        llm: Language model\n",
    "        top_k: Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (document, relevance_score) tuples\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔄 Semantic reranking {len(documents)} documents...\")\n",
    "    \n",
    "    scored_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # Simplified scoring: use LLM to rate relevance\n",
    "        # In production, use proper reranking models\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Rate the relevance of this document to the query on a scale of 0.0 to 1.0.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Respond with ONLY a number between 0.0 and 1.0, where:\n",
    "- 1.0 = Highly relevant, directly answers the query\n",
    "- 0.5 = Somewhat relevant, partially related\n",
    "- 0.0 = Not relevant\n",
    "\n",
    "Relevance score:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # For demo purposes, we'll use a simpler heuristic\n",
    "        # In production, call the LLM or use Cohere Rerank\n",
    "        \n",
    "        # Simplified: score based on keyword overlap + metadata\n",
    "        query_lower = query.lower()\n",
    "        doc_lower = doc.page_content.lower()\n",
    "        \n",
    "        # Keyword overlap score\n",
    "        query_words = set(query_lower.split())\n",
    "        doc_words = set(doc_lower.split())\n",
    "        overlap = len(query_words & doc_words) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # Boost score for title match\n",
    "        title_boost = 0.2 if any(word in doc.metadata.get('title', '').lower() for word in query_words) else 0\n",
    "        \n",
    "        score = min(overlap + title_boost, 1.0)\n",
    "        scored_docs.append((doc, score))\n",
    "    \n",
    "    # Sort by score\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"   ✓ Reranked, returning top {top_k}\\n\")\n",
    "    return scored_docs[:top_k]\n",
    "\n",
    "print(\"✅ Semantic reranking function created\")\n",
    "print(\"\\n💡 Note: This is a simplified demo. In production, use:\")\n",
    "print(\"   - Cohere Rerank API (highly recommended)\")\n",
    "print(\"   - sentence-transformers cross-encoders\")\n",
    "print(\"   - Specialized reranking models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Security-Specific Ranking Functions\n",
    "\n",
    "Now let's build ranking functions that prioritize by security-relevant metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def severity_score(doc: Document) -> float:\n",
    "    \"\"\"\n",
    "    Score document by severity/risk level.\n",
    "    \n",
    "    Returns:\n",
    "        Score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    risk_level = doc.metadata.get('risk_level', 'Medium')\n",
    "    \n",
    "    severity_map = {\n",
    "        'Critical': 1.0,\n",
    "        'High': 0.7,\n",
    "        'Medium': 0.4,\n",
    "        'Low': 0.2\n",
    "    }\n",
    "    \n",
    "    return severity_map.get(risk_level, 0.4)\n",
    "\n",
    "\n",
    "def recency_score(doc: Document, decay_days: int = 365) -> float:\n",
    "    \"\"\"\n",
    "    Score document by recency (newer = higher score).\n",
    "    \n",
    "    Args:\n",
    "        doc: Document with optional 'date_published' metadata\n",
    "        decay_days: Days for score to decay to 0.5\n",
    "        \n",
    "    Returns:\n",
    "        Score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # For demo, since we don't have dates, return neutral score\n",
    "    # In production, parse date_published and calculate age\n",
    "    \n",
    "    date_str = doc.metadata.get('date_published')\n",
    "    if not date_str:\n",
    "        return 0.5  # Neutral score if no date\n",
    "    \n",
    "    try:\n",
    "        pub_date = datetime.fromisoformat(date_str)\n",
    "        age_days = (datetime.now() - pub_date).days\n",
    "        \n",
    "        # Exponential decay: score = exp(-age / decay_days)\n",
    "        score = np.exp(-age_days / decay_days)\n",
    "        return score\n",
    "    except:\n",
    "        return 0.5  # Default if parsing fails\n",
    "\n",
    "\n",
    "def exploit_score(doc: Document) -> float:\n",
    "    \"\"\"\n",
    "    Score document by exploit availability.\n",
    "    \n",
    "    Returns:\n",
    "        1.0 if exploit available, 0.5 otherwise\n",
    "    \"\"\"\n",
    "    # Check if document mentions exploits\n",
    "    content_lower = doc.page_content.lower()\n",
    "    exploit_keywords = ['exploit', 'exploitation', 'actively exploited', 'in-the-wild']\n",
    "    \n",
    "    if any(keyword in content_lower for keyword in exploit_keywords):\n",
    "        return 1.0\n",
    "    return 0.5\n",
    "\n",
    "\n",
    "print(\"✅ Security-specific scoring functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Multi-Signal Ranking\n",
    "\n",
    "Combine multiple scoring signals with configurable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_signal_rerank(\n",
    "    query: str,\n",
    "    documents: List[Document],\n",
    "    weights: Optional[Dict[str, float]] = None,\n",
    "    top_k: int = 5\n",
    ") -> List[Tuple[Document, float, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Rerank documents using multiple signals.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        documents: Candidate documents (with similarity scores)\n",
    "        weights: Weight for each signal (default: equal weights)\n",
    "        top_k: Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (document, final_score, signal_scores) tuples\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            'semantic': 0.4,    # Relevance to query\n",
    "            'severity': 0.3,    # Risk level\n",
    "            'recency': 0.15,    # How recent\n",
    "            'exploit': 0.15     # Exploit availability\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n🔄 Multi-signal reranking with weights:\")\n",
    "    for signal, weight in weights.items():\n",
    "        print(f\"   {signal}: {weight:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    scored_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Calculate individual signal scores\n",
    "        signals = {\n",
    "            'semantic': 1.0,  # Assume normalized similarity (could use actual sim score)\n",
    "            'severity': severity_score(doc),\n",
    "            'recency': recency_score(doc),\n",
    "            'exploit': exploit_score(doc)\n",
    "        }\n",
    "        \n",
    "        # Weighted combination\n",
    "        final_score = sum(signals[key] * weights[key] for key in signals.keys())\n",
    "        \n",
    "        scored_docs.append((doc, final_score, signals))\n",
    "    \n",
    "    # Sort by final score\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"✓ Reranked, returning top {top_k}\\n\")\n",
    "    return scored_docs[:top_k]\n",
    "\n",
    "print(\"✅ Multi-signal reranking function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Demonstrations\n",
    "\n",
    "Let's compare different reranking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"What are the most critical security risks?\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"❓ Query: {test_query}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initial retrieval\n",
    "print(\"\\n1️⃣  INITIAL RETRIEVAL (Similarity Only)\")\n",
    "print(\"-\"*80)\n",
    "initial_docs = vectorstore.similarity_search(test_query, k=5)\n",
    "print(f\"Retrieved {len(initial_docs)} documents:\\n\")\n",
    "for i, doc in enumerate(initial_docs, 1):\n",
    "    print(f\"{i}. {doc.metadata.get('id')}: {doc.metadata.get('title')}\")\n",
    "    print(f\"   Risk: {doc.metadata.get('risk_level')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-signal reranking\n",
    "print(\"\\n2️⃣  MULTI-SIGNAL RERANKING\")\n",
    "print(\"-\"*80)\n",
    "reranked = multi_signal_rerank(test_query, initial_docs, top_k=5)\n",
    "\n",
    "print(\"Reranked results:\\n\")\n",
    "for i, (doc, final_score, signals) in enumerate(reranked, 1):\n",
    "    print(f\"{i}. {doc.metadata.get('id')}: {doc.metadata.get('title')}\")\n",
    "    print(f\"   Risk: {doc.metadata.get('risk_level')}\")\n",
    "    print(f\"   Final Score: {final_score:.3f}\")\n",
    "    print(f\"   Signals: severity={signals['severity']:.2f}, recency={signals['recency']:.2f}, exploit={signals['exploit']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Weights Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heavily weight severity for security-critical applications\n",
    "security_focused_weights = {\n",
    "    'semantic': 0.2,    # Less emphasis on semantic match\n",
    "    'severity': 0.5,    # Heavily prioritize severity\n",
    "    'recency': 0.1,     # Less emphasis on recency\n",
    "    'exploit': 0.2      # Moderate emphasis on exploits\n",
    "}\n",
    "\n",
    "print(\"\\n3️⃣  SECURITY-FOCUSED RERANKING\")\n",
    "print(\"-\"*80)\n",
    "security_reranked = multi_signal_rerank(\n",
    "    test_query, \n",
    "    initial_docs, \n",
    "    weights=security_focused_weights,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"Security-focused reranking results:\\n\")\n",
    "for i, (doc, final_score, signals) in enumerate(security_reranked, 1):\n",
    "    print(f\"{i}. {doc.metadata.get('id')}: {doc.metadata.get('title')}\")\n",
    "    print(f\"   Risk: {doc.metadata.get('risk_level')}\")\n",
    "    print(f\"   Final Score: {final_score:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Complete RAG Pipeline with Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_reranking(\n",
    "    query: str,\n",
    "    vectorstore: Chroma,\n",
    "    llm,\n",
    "    rerank_method: str = 'multi_signal',\n",
    "    weights: Optional[Dict[str, float]] = None,\n",
    "    k_retrieve: int = 10,\n",
    "    k_final: int = 3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with reranking.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        vectorstore: Vector store\n",
    "        llm: Language model\n",
    "        rerank_method: 'multi_signal', 'semantic', or 'none'\n",
    "        weights: Custom weights for multi_signal\n",
    "        k_retrieve: Number of candidates to retrieve\n",
    "        k_final: Number of documents to use for generation\n",
    "        \n",
    "    Returns:\n",
    "        Generated answer\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🔍 RAG with Reranking: {rerank_method}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Step 1: Initial retrieval (over-fetch)\n",
    "    print(f\"1️⃣  Retrieving {k_retrieve} candidate documents...\")\n",
    "    candidates = vectorstore.similarity_search(query, k=k_retrieve)\n",
    "    print(f\"   Retrieved {len(candidates)} candidates\\n\")\n",
    "    \n",
    "    # Step 2: Rerank\n",
    "    print(f\"2️⃣  Reranking with method: {rerank_method}\")\n",
    "    \n",
    "    if rerank_method == 'multi_signal':\n",
    "        reranked = multi_signal_rerank(query, candidates, weights, k_final)\n",
    "        final_docs = [doc for doc, score, signals in reranked]\n",
    "    elif rerank_method == 'semantic':\n",
    "        reranked = semantic_rerank(query, candidates, llm, k_final)\n",
    "        final_docs = [doc for doc, score in reranked]\n",
    "    else:\n",
    "        # No reranking\n",
    "        final_docs = candidates[:k_final]\n",
    "        print(f\"   No reranking applied\\n\")\n",
    "    \n",
    "    print(f\"   Selected top {len(final_docs)} documents for generation\\n\")\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    print(\"3️⃣  Generating answer...\\n\")\n",
    "    \n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1} ({doc.metadata['id']} - {doc.metadata['title']}, Risk: {doc.metadata['risk_level']}):\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(final_docs)\n",
    "    ])\n",
    "    \n",
    "    answer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are an AI security expert assistant.\n",
    "\n",
    "Use the following security documentation to answer the user's question.\n",
    "The documents have been ranked by both relevance and security priority.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Provide a comprehensive answer based on the context\n",
    "2. Prioritize information from higher-risk vulnerabilities\n",
    "3. Cite specific vulnerabilities and risk levels\n",
    "4. Include prevention measures and best practices\n",
    "\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    prompt_value = answer_prompt.invoke({\"context\": context, \"question\": query})\n",
    "    response = llm.invoke(prompt_value)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "print(\"✅ Complete RAG with reranking pipeline created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete pipeline\n",
    "query = \"What are the most critical security risks for LLM applications?\"\n",
    "\n",
    "answer = rag_with_reranking(\n",
    "    query=query,\n",
    "    vectorstore=vectorstore,\n",
    "    llm=llm,\n",
    "    rerank_method='multi_signal',\n",
    "    k_retrieve=10,\n",
    "    k_final=3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📄 ANSWER\")\n",
    "print(\"=\"*80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Comparison: Different Reranking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_reranking_strategies(query: str, vectorstore: Chroma):\n",
    "    \"\"\"\n",
    "    Compare different reranking strategies.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"❓ Query: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Retrieve candidates\n",
    "    candidates = vectorstore.similarity_search(query, k=10)\n",
    "    \n",
    "    # Strategy 1: No reranking (baseline)\n",
    "    print(\"\\n1️⃣  NO RERANKING (Baseline)\")\n",
    "    print(\"-\"*80)\n",
    "    baseline = candidates[:5]\n",
    "    print(\"Top 5 documents:\\n\")\n",
    "    for i, doc in enumerate(baseline, 1):\n",
    "        print(f\"{i}. {doc.metadata.get('id')}: {doc.metadata.get('title')} (Risk: {doc.metadata.get('risk_level')})\")\n",
    "    \n",
    "    # Strategy 2: Multi-signal (balanced)\n",
    "    print(\"\\n2️⃣  MULTI-SIGNAL (Balanced Weights)\")\n",
    "    print(\"-\"*80)\n",
    "    balanced = multi_signal_rerank(query, candidates, top_k=5)\n",
    "    print(\"Top 5 documents:\\n\")\n",
    "    for i, (doc, score, signals) in enumerate(balanced, 1):\n",
    "        print(f\"{i}. {doc.metadata.get('id')}: {doc.metadata.get('title')} (Risk: {doc.metadata.get('risk_level')}, Score: {score:.3f})\")\n",
    "    \n",
    "    # Strategy 3: Severity-focused\n",
    "    print(\"\\n3️⃣  SEVERITY-FOCUSED\")\n",
    "    print(\"-\"*80)\n",
    "    severity_weights = {'semantic': 0.2, 'severity': 0.5, 'recency': 0.1, 'exploit': 0.2}\n",
    "    severity_focused = multi_signal_rerank(query, candidates, weights=severity_weights, top_k=5)\n",
    "    print(\"Top 5 documents:\\n\")\n",
    "    for i, (doc, score, signals) in enumerate(severity_focused, 1):\n",
    "        print(f\"{i}. {doc.metadata.get('id')}: {doc.metadata.get('title')} (Risk: {doc.metadata.get('risk_level')}, Score: {score:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"✅ Notice how different strategies prioritize different documents\")\n",
    "    print(\"✅ Severity-focused ranking surfaces Critical/High risks first\")\n",
    "    print(\"✅ Choose weights based on your use case and priorities\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"✅ Comparison function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "compare_reranking_strategies(\n",
    "    \"What security vulnerabilities should I prioritize?\",\n",
    "    vectorstore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Production Best Practices\n",
    "\n",
    "### Reranking Strategy Selection\n",
    "\n",
    "**Choose based on use case:**\n",
    "\n",
    "1. **No Reranking (Baseline)**\n",
    "   - Use when: Semantic relevance is all that matters\n",
    "   - Pros: Fastest, simplest\n",
    "   - Cons: May not prioritize by business impact\n",
    "\n",
    "2. **Multi-Signal Reranking**\n",
    "   - Use when: Need to balance relevance + metadata\n",
    "   - Pros: Configurable, explainable, fast\n",
    "   - Cons: Requires tuning weights\n",
    "\n",
    "3. **Cross-Encoder Reranking**\n",
    "   - Use when: Quality > speed, semantic nuance matters\n",
    "   - Pros: Most accurate\n",
    "   - Cons: Slower, more expensive\n",
    "\n",
    "4. **Hybrid (Two-Stage)**\n",
    "   - Use when: Need both speed and quality\n",
    "   - Approach: Bi-encoder retrieval → Multi-signal filter → Cross-encoder rerank top-5\n",
    "   - Pros: Best of all worlds\n",
    "   - Cons: Most complex\n",
    "\n",
    "### Weight Tuning Guidelines\n",
    "\n",
    "**For different scenarios:**\n",
    "\n",
    "```python\n",
    "# Security-critical production system\n",
    "production_weights = {\n",
    "    'semantic': 0.2,\n",
    "    'severity': 0.5,    # Heavily prioritize critical issues\n",
    "    'recency': 0.1,\n",
    "    'exploit': 0.2      # Active exploits are urgent\n",
    "}\n",
    "\n",
    "# Research/analysis\n",
    "research_weights = {\n",
    "    'semantic': 0.6,    # Relevance is key\n",
    "    'severity': 0.2,\n",
    "    'recency': 0.1,\n",
    "    'exploit': 0.1\n",
    "}\n",
    "\n",
    "# Compliance/audit\n",
    "compliance_weights = {\n",
    "    'semantic': 0.3,\n",
    "    'severity': 0.4,\n",
    "    'recency': 0.2,     # Recent changes matter\n",
    "    'exploit': 0.1\n",
    "}\n",
    "```\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "1. **Over-fetch then rerank**: Retrieve 2-5x more than needed\n",
    "2. **Cache reranking results**: Cache for repeated queries\n",
    "3. **Batch reranking**: Process multiple queries together\n",
    "4. **Async reranking**: Don't block on slow rerankers\n",
    "5. **Fallback strategy**: If reranking fails, use baseline\n",
    "\n",
    "### Monitoring & Evaluation\n",
    "\n",
    "Track these metrics:\n",
    "- **Ranking quality**: NDCG, MRR, Precision@K\n",
    "- **Latency**: P50, P95, P99 for reranking time\n",
    "- **User feedback**: Click-through rate on reranked results\n",
    "- **Coverage**: % of queries where reranking changes order\n",
    "- **Signal distribution**: How often each signal influences ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "\n",
    "✅ Complete reranking system:\n",
    "1. **RRF Review**: Reciprocal Rank Fusion from Part 3\n",
    "2. **Semantic Reranking**: Cross-encoder style (simplified)\n",
    "3. **Security-Specific Scoring**: Severity, recency, exploit status\n",
    "4. **Multi-Signal Ranking**: Weighted combination of signals\n",
    "5. **Complete RAG Pipeline**: End-to-end with reranking\n",
    "6. **Comparison Framework**: Evaluate different strategies\n",
    "\n",
    "### Core Concepts Learned\n",
    "\n",
    "1. **Why Reranking Matters**: Semantic similarity ≠ business priority\n",
    "2. **Two-Stage Retrieval**: Fast bi-encoder → slow but accurate reranker\n",
    "3. **Multi-Signal Scoring**: Combine semantic + metadata signals\n",
    "4. **Configurable Weights**: Tune for different use cases\n",
    "5. **Production Patterns**: Over-fetch, cache, fallback\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "**Reranking Benefits:**\n",
    "- ↑↑ Prioritize by business importance, not just relevance\n",
    "- ↑ Better user experience (right results first)\n",
    "- ↑ Configurable and explainable\n",
    "- ✅ Essential for security/compliance applications\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Accuracy vs Speed**: Cross-encoders are slow but accurate\n",
    "- **Complexity vs Control**: More signals = more tuning\n",
    "- **Generality vs Specificity**: Custom scoring works for domain\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "1. **Start with multi-signal**: Simple, fast, configurable\n",
    "2. **Tune weights for your domain**: Security ≠ e-commerce\n",
    "3. **Monitor ranking quality**: Track user engagement\n",
    "4. **Use two-stage retrieval**: Over-fetch then rerank\n",
    "5. **Implement fallbacks**: Don't fail if reranking breaks\n",
    "6. **A/B test weights**: Optimize based on real usage\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Part 7**, we'll implement **RAPTOR**:\n",
    "- Hierarchical knowledge organization\n",
    "- Recursive summarization\n",
    "- Tree-based retrieval\n",
    "- Multi-level abstraction (tactics → techniques → procedures)\n",
    "\n",
    "Example: Navigate MITRE ATT&CK hierarchy from high-level tactics down to specific procedures.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Practice Exercises\n",
    "\n",
    "1. **Integrate Cohere Rerank**: Use real reranking API\n",
    "2. **Add More Signals**: Implement affected_products, cwe_id scoring\n",
    "3. **Learn-to-Rank**: Use ML to learn optimal weights\n",
    "4. **Dynamic Weights**: Adjust weights based on query type\n",
    "5. **Ranking Explainability**: Show users why docs were ranked\n",
    "\n",
    "### 📚 Further Reading\n",
    "\n",
    "- [Cohere Rerank Documentation](https://docs.cohere.com/docs/reranking)\n",
    "- [Cross-Encoders for Reranking](https://www.sbert.net/examples/applications/cross-encoder/README.html)\n",
    "- [Learning to Rank](https://en.wikipedia.org/wiki/Learning_to_rank)\n",
    "- [NDCG and Ranking Metrics](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
