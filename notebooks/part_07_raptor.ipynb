{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Hierarchical Security Knowledge (RAPTOR)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)\n",
    "2. Build hierarchical knowledge structures for security content\n",
    "3. Implement recursive summarization at multiple levels\n",
    "4. Create multi-level embeddings and indexing\n",
    "5. Retrieve at different abstraction levels\n",
    "6. Navigate from high-level concepts to specific details\n",
    "7. Organize MITRE ATT&CK and OWASP in hierarchies\n",
    "\n",
    "## The Problem with Flat Document Retrieval\n",
    "\n",
    "Traditional RAG treats all documents equally at one level. This has limitations for hierarchical knowledge:\n",
    "\n",
    "### Example: MITRE ATT&CK Framework\n",
    "\n",
    "**Natural hierarchy:**\n",
    "```\n",
    "Tactic: Credential Access\n",
    "├── Technique: OS Credential Dumping (T1003)\n",
    "│   ├── Sub-technique: LSASS Memory (T1003.001)\n",
    "│   ├── Sub-technique: Security Account Manager (T1003.002)\n",
    "│   └── Sub-technique: NTDS (T1003.003)\n",
    "├── Technique: Brute Force (T1110)\n",
    "│   ├── Sub-technique: Password Guessing (T1110.001)\n",
    "│   └── Sub-technique: Password Cracking (T1110.002)\n",
    "└── ...\n",
    "```\n",
    "\n",
    "**Problem with flat retrieval:**\n",
    "- Query: \"What are credential access techniques?\" → May retrieve specific sub-techniques without tactical context\n",
    "- Query: \"How does LSASS dumping work?\" → May miss the broader credential dumping context\n",
    "- No way to navigate up/down the hierarchy\n",
    "- No high-level summaries available\n",
    "\n",
    "## Solution: RAPTOR\n",
    "\n",
    "**RAPTOR** builds a tree of abstractions:\n",
    "\n",
    "### Key Ideas:\n",
    "\n",
    "1. **Recursive Summarization**: Create summaries at multiple levels\n",
    "2. **Tree Organization**: Organize knowledge in a hierarchy\n",
    "3. **Multi-Level Retrieval**: Query at the appropriate abstraction level\n",
    "4. **Context Preservation**: Maintain parent-child relationships\n",
    "\n",
    "### RAPTOR Tree Structure:\n",
    "\n",
    "```\n",
    "Level 0 (Original Documents):\n",
    "[Doc1: LSASS details] [Doc2: SAM details] [Doc3: NTDS details] ...\n",
    "\n",
    "        ↓ Cluster + Summarize\n",
    "\n",
    "Level 1 (Technique Summaries):\n",
    "[Summary: OS Credential Dumping methods] [Summary: Brute Force methods] ...\n",
    "\n",
    "        ↓ Cluster + Summarize\n",
    "\n",
    "Level 2 (Tactic Summary):\n",
    "[Summary: Credential Access techniques overview]\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- ✅ High-level queries get summaries\n",
    "- ✅ Specific queries get details\n",
    "- ✅ Navigate hierarchy (drill down/up)\n",
    "- ✅ Better context understanding\n",
    "- ✅ Scalable to large knowledge bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies\n",
    "!pip install -q scikit-learn umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Clustering and dimensionality reduction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️  WARNING: OPENAI_API_KEY not found\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings and LLM\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"✅ Embeddings and LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our existing vector store\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"owasp_llm_top10\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../data/chroma_db\"\n",
    ")\n",
    "\n",
    "# Get all documents from vector store\n",
    "all_docs = vectorstore.similarity_search(\"\", k=100)  # Get all documents\n",
    "\n",
    "print(\"✅ Vector store loaded\")\n",
    "print(f\"   Total documents: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. RAPTOR Overview\n",
    "\n",
    "### Algorithm Steps:\n",
    "\n",
    "1. **Start with leaf nodes** (original documents)\n",
    "2. **Cluster** similar documents\n",
    "3. **Summarize** each cluster → creates parent nodes\n",
    "4. **Embed** summaries\n",
    "5. **Repeat** on parent nodes until tree converges\n",
    "\n",
    "### Tree Node Structure:\n",
    "\n",
    "```python\n",
    "class TreeNode:\n",
    "    content: str              # Text content or summary\n",
    "    embedding: List[float]    # Vector embedding\n",
    "    level: int                # 0=leaf, 1=first summary, etc.\n",
    "    children: List[TreeNode]  # Child nodes\n",
    "    parent: Optional[TreeNode] # Parent node\n",
    "    metadata: Dict            # Original metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Building the RAPTOR Tree\n",
    "\n",
    "Let's implement the core RAPTOR algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAPTORNode:\n",
    "    \"\"\"Node in the RAPTOR tree.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        content: str,\n",
    "        embedding: Optional[np.ndarray] = None,\n",
    "        level: int = 0,\n",
    "        metadata: Optional[Dict] = None,\n",
    "        children: Optional[List['RAPTORNode']] = None\n",
    "    ):\n",
    "        self.content = content\n",
    "        self.embedding = embedding\n",
    "        self.level = level\n",
    "        self.metadata = metadata or {}\n",
    "        self.children = children or []\n",
    "        self.parent = None\n",
    "        \n",
    "        # Set parent references for children\n",
    "        for child in self.children:\n",
    "            child.parent = self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"RAPTORNode(level={self.level}, children={len(self.children)}, content_len={len(self.content)})\"\n",
    "\n",
    "print(\"✅ RAPTORNode class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_documents(\n",
    "    embeddings_array: np.ndarray,\n",
    "    n_clusters: Optional[int] = None,\n",
    "    min_cluster_size: int = 2\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cluster documents based on embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_array: Array of embeddings (n_docs, embedding_dim)\n",
    "        n_clusters: Number of clusters (if None, auto-determine)\n",
    "        min_cluster_size: Minimum documents per cluster\n",
    "        \n",
    "    Returns:\n",
    "        Cluster labels for each document\n",
    "    \"\"\"\n",
    "    n_docs = embeddings_array.shape[0]\n",
    "    \n",
    "    # Auto-determine number of clusters if not specified\n",
    "    if n_clusters is None:\n",
    "        # Use sqrt(n) as heuristic, bounded by reasonable limits\n",
    "        n_clusters = max(2, min(int(np.sqrt(n_docs)), n_docs // min_cluster_size))\n",
    "    \n",
    "    # Ensure we have enough documents for clustering\n",
    "    if n_docs < n_clusters * min_cluster_size:\n",
    "        n_clusters = max(1, n_docs // min_cluster_size)\n",
    "    \n",
    "    if n_clusters <= 1:\n",
    "        # Too few documents, return single cluster\n",
    "        return np.zeros(n_docs, dtype=int)\n",
    "    \n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(embeddings_array)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "print(\"✅ Clustering function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cluster(\n",
    "    documents: List[str],\n",
    "    llm,\n",
    "    max_length: int = 500\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a summary of a cluster of documents.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document texts to summarize\n",
    "        llm: Language model\n",
    "        max_length: Maximum summary length in words\n",
    "        \n",
    "    Returns:\n",
    "        Summary text\n",
    "    \"\"\"\n",
    "    # Combine documents\n",
    "    combined = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(documents)])\n",
    "    \n",
    "    # Create summary prompt\n",
    "    summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are summarizing security documentation for a hierarchical knowledge system.\n",
    "\n",
    "Create a comprehensive summary of the following related security documents.\n",
    "The summary should:\n",
    "1. Capture the main themes and key concepts\n",
    "2. Preserve important security details (vulnerabilities, mitigations, risks)\n",
    "3. Be self-contained and understandable without the original documents\n",
    "4. Be approximately {max_length} words\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    )\n",
    "    \n",
    "    prompt_value = summary_prompt.invoke({\"documents\": combined, \"max_length\": max_length})\n",
    "    response = llm.invoke(prompt_value)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "print(\"✅ Summarization function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_raptor_tree(\n",
    "    documents: List[Document],\n",
    "    embeddings_model,\n",
    "    llm,\n",
    "    max_levels: int = 3,\n",
    "    min_cluster_size: int = 2\n",
    ") -> List[List[RAPTORNode]]:\n",
    "    \"\"\"\n",
    "    Build a RAPTOR tree from documents.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of documents to organize\n",
    "        embeddings_model: Embedding model\n",
    "        llm: Language model for summarization\n",
    "        max_levels: Maximum tree depth\n",
    "        min_cluster_size: Minimum documents per cluster\n",
    "        \n",
    "    Returns:\n",
    "        List of levels, each containing RAPTORNodes\n",
    "    \"\"\"\n",
    "    print(f\"\\n🌲 Building RAPTOR tree from {len(documents)} documents...\\n\")\n",
    "    \n",
    "    # Level 0: Create leaf nodes from original documents\n",
    "    print(\"📄 Level 0: Creating leaf nodes...\")\n",
    "    \n",
    "    # Get embeddings for all documents\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    doc_embeddings = embeddings_model.embed_documents(texts)\n",
    "    \n",
    "    leaf_nodes = [\n",
    "        RAPTORNode(\n",
    "            content=doc.page_content,\n",
    "            embedding=np.array(emb),\n",
    "            level=0,\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        for doc, emb in zip(documents, doc_embeddings)\n",
    "    ]\n",
    "    \n",
    "    print(f\"   Created {len(leaf_nodes)} leaf nodes\\n\")\n",
    "    \n",
    "    # Build tree levels\n",
    "    tree_levels = [leaf_nodes]\n",
    "    current_nodes = leaf_nodes\n",
    "    \n",
    "    for level in range(1, max_levels + 1):\n",
    "        print(f\"📊 Level {level}: Clustering and summarizing...\")\n",
    "        \n",
    "        # Check if we have enough nodes to continue\n",
    "        if len(current_nodes) < min_cluster_size:\n",
    "            print(f\"   Too few nodes ({len(current_nodes)}), stopping tree construction\\n\")\n",
    "            break\n",
    "        \n",
    "        # Get embeddings matrix\n",
    "        embeddings_matrix = np.array([node.embedding for node in current_nodes])\n",
    "        \n",
    "        # Cluster nodes\n",
    "        labels = cluster_documents(embeddings_matrix, min_cluster_size=min_cluster_size)\n",
    "        n_clusters = len(set(labels))\n",
    "        \n",
    "        print(f\"   Clustered {len(current_nodes)} nodes into {n_clusters} groups\")\n",
    "        \n",
    "        # Create parent nodes by summarizing clusters\n",
    "        parent_nodes = []\n",
    "        \n",
    "        for cluster_id in range(n_clusters):\n",
    "            # Get nodes in this cluster\n",
    "            cluster_indices = np.where(labels == cluster_id)[0]\n",
    "            cluster_nodes = [current_nodes[i] for i in cluster_indices]\n",
    "            \n",
    "            if len(cluster_nodes) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Summarize cluster\n",
    "            cluster_texts = [node.content for node in cluster_nodes]\n",
    "            summary = summarize_cluster(cluster_texts, llm)\n",
    "            \n",
    "            # Embed summary\n",
    "            summary_embedding = embeddings_model.embed_query(summary)\n",
    "            \n",
    "            # Create parent node\n",
    "            parent_node = RAPTORNode(\n",
    "                content=summary,\n",
    "                embedding=np.array(summary_embedding),\n",
    "                level=level,\n",
    "                children=cluster_nodes,\n",
    "                metadata={'cluster_id': cluster_id, 'n_children': len(cluster_nodes)}\n",
    "            )\n",
    "            \n",
    "            parent_nodes.append(parent_node)\n",
    "        \n",
    "        print(f\"   Created {len(parent_nodes)} parent nodes\\n\")\n",
    "        \n",
    "        # Add level to tree\n",
    "        tree_levels.append(parent_nodes)\n",
    "        current_nodes = parent_nodes\n",
    "        \n",
    "        # Stop if we've converged to a single node or too few nodes\n",
    "        if len(parent_nodes) <= 1:\n",
    "            print(f\"✅ Tree converged at level {level}\\n\")\n",
    "            break\n",
    "    \n",
    "    print(f\"🌲 RAPTOR tree built with {len(tree_levels)} levels\")\n",
    "    for i, level in enumerate(tree_levels):\n",
    "        print(f\"   Level {i}: {len(level)} nodes\")\n",
    "    print()\n",
    "    \n",
    "    return tree_levels\n",
    "\n",
    "print(\"✅ RAPTOR tree building function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Build RAPTOR Tree from Our Security Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAPTOR tree\n",
    "raptor_tree = build_raptor_tree(\n",
    "    documents=all_docs,\n",
    "    embeddings_model=embeddings,\n",
    "    llm=llm,\n",
    "    max_levels=2,  # Build 2 levels of summaries (3 total levels including leaves)\n",
    "    min_cluster_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the tree structure\n",
    "print(\"\\n📊 RAPTOR Tree Structure:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for level_idx, level_nodes in enumerate(raptor_tree):\n",
    "    print(f\"\\nLevel {level_idx}: {len(level_nodes)} nodes\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i, node in enumerate(level_nodes[:3]):  # Show first 3 nodes per level\n",
    "        print(f\"\\nNode {i+1}:\")\n",
    "        print(f\"  Content preview: {node.content[:150]}...\")\n",
    "        print(f\"  Children: {len(node.children)}\")\n",
    "        if node.metadata:\n",
    "            print(f\"  Metadata: {node.metadata}\")\n",
    "    \n",
    "    if len(level_nodes) > 3:\n",
    "        print(f\"\\n... and {len(level_nodes) - 3} more nodes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Hierarchical Retrieval Strategies\n",
    "\n",
    "Now we can query at different abstraction levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raptor_retrieval(\n",
    "    query: str,\n",
    "    tree_levels: List[List[RAPTORNode]],\n",
    "    embeddings_model,\n",
    "    strategy: str = 'auto',\n",
    "    k: int = 3\n",
    ") -> List[RAPTORNode]:\n",
    "    \"\"\"\n",
    "    Retrieve from RAPTOR tree using different strategies.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        tree_levels: RAPTOR tree levels\n",
    "        embeddings_model: Embedding model\n",
    "        strategy: 'leaves', 'summaries', 'top', 'auto', 'all_levels'\n",
    "        k: Number of nodes to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved RAPTORNodes\n",
    "    \"\"\"\n",
    "    # Embed query\n",
    "    query_embedding = np.array(embeddings_model.embed_query(query))\n",
    "    \n",
    "    # Select levels based on strategy\n",
    "    if strategy == 'leaves':\n",
    "        # Only search leaf nodes (original documents)\n",
    "        search_levels = [0]\n",
    "    elif strategy == 'summaries':\n",
    "        # Only search summary nodes (level 1+)\n",
    "        search_levels = list(range(1, len(tree_levels)))\n",
    "    elif strategy == 'top':\n",
    "        # Only search top level (most abstract)\n",
    "        search_levels = [len(tree_levels) - 1]\n",
    "    elif strategy == 'all_levels':\n",
    "        # Search all levels\n",
    "        search_levels = list(range(len(tree_levels)))\n",
    "    else:  # 'auto'\n",
    "        # Determine level based on query complexity\n",
    "        # Simple heuristic: longer queries = more specific = search leaves\n",
    "        query_words = query.split()\n",
    "        if len(query_words) <= 5:\n",
    "            # Short query = high-level = search summaries\n",
    "            search_levels = [len(tree_levels) - 1, len(tree_levels) - 2] if len(tree_levels) > 1 else [0]\n",
    "        else:\n",
    "            # Long query = specific = search leaves and level 1\n",
    "            search_levels = [0, 1] if len(tree_levels) > 1 else [0]\n",
    "    \n",
    "    # Remove invalid level indices\n",
    "    search_levels = [l for l in search_levels if l < len(tree_levels)]\n",
    "    \n",
    "    print(f\"\\n🔍 RAPTOR Retrieval Strategy: {strategy}\")\n",
    "    print(f\"   Searching levels: {search_levels}\")\n",
    "    \n",
    "    # Collect all candidate nodes from selected levels\n",
    "    candidates = []\n",
    "    for level_idx in search_levels:\n",
    "        candidates.extend(tree_levels[level_idx])\n",
    "    \n",
    "    print(f\"   Total candidates: {len(candidates)}\")\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    scored_nodes = []\n",
    "    for node in candidates:\n",
    "        similarity = np.dot(query_embedding, node.embedding) / (\n",
    "            np.linalg.norm(query_embedding) * np.linalg.norm(node.embedding)\n",
    "        )\n",
    "        scored_nodes.append((node, similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    scored_nodes.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top k\n",
    "    top_nodes = [node for node, score in scored_nodes[:k]]\n",
    "    \n",
    "    print(f\"   Retrieved top {len(top_nodes)} nodes\\n\")\n",
    "    \n",
    "    return top_nodes\n",
    "\n",
    "print(\"✅ RAPTOR retrieval function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Demonstrations\n",
    "\n",
    "Let's test hierarchical retrieval with different query types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: High-Level Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_level_query = \"What are LLM security risks?\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"❓ High-Level Query: {high_level_query}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try different strategies\n",
    "for strategy in ['top', 'auto', 'leaves']:\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"Strategy: {strategy}\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    results = raptor_retrieval(\n",
    "        query=high_level_query,\n",
    "        tree_levels=raptor_tree,\n",
    "        embeddings_model=embeddings,\n",
    "        strategy=strategy,\n",
    "        k=2\n",
    "    )\n",
    "    \n",
    "    for i, node in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Level {node.level} Node:\")\n",
    "        print(f\"   Content: {node.content[:200]}...\")\n",
    "        print(f\"   Children: {len(node.children)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Specific Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_query = \"How do I prevent prompt injection attacks in my LLM application?\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"❓ Specific Query: {specific_query}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Auto strategy should search leaves for specific queries\n",
    "results = raptor_retrieval(\n",
    "    query=specific_query,\n",
    "    tree_levels=raptor_tree,\n",
    "    embeddings_model=embeddings,\n",
    "    strategy='auto',\n",
    "    k=3\n",
    ")\n",
    "\n",
    "for i, node in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Level {node.level} Node:\")\n",
    "    print(f\"   Content: {node.content[:300]}...\")\n",
    "    if node.metadata.get('title'):\n",
    "        print(f\"   Title: {node.metadata['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Complete RAG with RAPTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_raptor(\n",
    "    query: str,\n",
    "    tree_levels: List[List[RAPTORNode]],\n",
    "    embeddings_model,\n",
    "    llm,\n",
    "    strategy: str = 'auto',\n",
    "    k: int = 3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline using RAPTOR hierarchical retrieval.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🌲 RAG with RAPTOR\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Retrieve nodes\n",
    "    nodes = raptor_retrieval(query, tree_levels, embeddings_model, strategy, k)\n",
    "    \n",
    "    # Format context\n",
    "    context_parts = []\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        level_desc = \"Detail\" if node.level == 0 else f\"Summary (Level {node.level})\"\n",
    "        context_parts.append(f\"[{level_desc}]:\\n{node.content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Generate answer\n",
    "    print(\"📝 Generating answer...\\n\")\n",
    "    \n",
    "    answer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are an AI security expert assistant using hierarchical knowledge retrieval.\n",
    "\n",
    "The context below includes both high-level summaries and detailed information retrieved from different abstraction levels.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Provide a comprehensive answer using the multi-level context\n",
    "2. Start with high-level overview if summaries are present\n",
    "3. Include specific details when available\n",
    "4. Maintain logical flow from general to specific\n",
    "\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    prompt_value = answer_prompt.invoke({\"context\": context, \"question\": query})\n",
    "    response = llm.invoke(prompt_value)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "print(\"✅ RAG with RAPTOR pipeline created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete RAG with RAPTOR\n",
    "query = \"What are the main security concerns for LLM applications?\"\n",
    "\n",
    "answer = rag_with_raptor(\n",
    "    query=query,\n",
    "    tree_levels=raptor_tree,\n",
    "    embeddings_model=embeddings,\n",
    "    llm=llm,\n",
    "    strategy='auto',\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📄 ANSWER\")\n",
    "print(\"=\"*80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Comparison: RAPTOR vs Flat Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_raptor_vs_flat(query: str, vectorstore, tree_levels, embeddings_model, llm):\n",
    "    \"\"\"\n",
    "    Compare RAPTOR hierarchical retrieval vs flat vector search.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"❓ Query: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Flat retrieval\n",
    "    print(\"\\n1️⃣  FLAT VECTOR SEARCH (Baseline)\")\n",
    "    print(\"-\"*80)\n",
    "    flat_docs = vectorstore.similarity_search(query, k=3)\n",
    "    print(f\"Retrieved {len(flat_docs)} documents:\\n\")\n",
    "    for i, doc in enumerate(flat_docs, 1):\n",
    "        print(f\"{i}. {doc.metadata.get('id')}: {doc.metadata.get('title')}\")\n",
    "        print(f\"   Preview: {doc.page_content[:150]}...\\n\")\n",
    "    \n",
    "    # RAPTOR retrieval\n",
    "    print(\"\\n2️⃣  RAPTOR HIERARCHICAL RETRIEVAL\")\n",
    "    print(\"-\"*80)\n",
    "    raptor_nodes = raptor_retrieval(query, tree_levels, embeddings_model, strategy='auto', k=3)\n",
    "    print(f\"\\nRetrieved {len(raptor_nodes)} nodes:\\n\")\n",
    "    for i, node in enumerate(raptor_nodes, 1):\n",
    "        level_desc = \"Original\" if node.level == 0 else f\"Summary L{node.level}\"\n",
    "        print(f\"{i}. [{level_desc}] {len(node.children)} children\")\n",
    "        print(f\"   Content: {node.content[:150]}...\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"✅ Flat retrieval: Returns only original documents at one level\")\n",
    "    print(\"✅ RAPTOR: Can return summaries for high-level queries, details for specific queries\")\n",
    "    print(\"✅ RAPTOR: Better context through hierarchical organization\")\n",
    "    print(\"✅ RAPTOR: Navigate from overview to specifics\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"✅ Comparison function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "compare_raptor_vs_flat(\n",
    "    query=\"What are LLM vulnerabilities?\",\n",
    "    vectorstore=vectorstore,\n",
    "    tree_levels=raptor_tree,\n",
    "    embeddings_model=embeddings,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "\n",
    "✅ Complete RAPTOR implementation:\n",
    "1. **Hierarchical Tree Structure**: Multi-level organization\n",
    "2. **Recursive Summarization**: Cluster + summarize at each level\n",
    "3. **Multi-Level Embeddings**: Index at all abstraction levels\n",
    "4. **Flexible Retrieval Strategies**: Auto, top, summaries, leaves, all_levels\n",
    "5. **Complete RAG Pipeline**: End-to-end with RAPTOR\n",
    "6. **Comparison Framework**: RAPTOR vs flat retrieval\n",
    "\n",
    "### Core Concepts Learned\n",
    "\n",
    "1. **Hierarchical Knowledge**: Organize information at multiple abstraction levels\n",
    "2. **Recursive Processing**: Bottom-up tree construction\n",
    "3. **Clustering**: Group similar documents automatically\n",
    "4. **Abstractive Summarization**: Create higher-level representations\n",
    "5. **Multi-Level Retrieval**: Query at appropriate abstraction\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "**RAPTOR Benefits:**\n",
    "- ↑↑ Better for hierarchical knowledge (MITRE ATT&CK, OWASP)\n",
    "- ↑ Supports both high-level and specific queries\n",
    "- ↑ Navigate from overview to details\n",
    "- ↑ Better context understanding\n",
    "- ✅ Essential for large, structured knowledge bases\n",
    "\n",
    "**When to Use RAPTOR:**\n",
    "- Knowledge has natural hierarchy\n",
    "- Users need both overviews and details\n",
    "- Large document collections (100+)\n",
    "- Multi-level navigation required\n",
    "\n",
    "**When NOT to Use RAPTOR:**\n",
    "- Small document collections (<50)\n",
    "- Flat knowledge structure\n",
    "- Only specific queries (no high-level)\n",
    "- Latency-sensitive (tree building is slow)\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "1. **Pre-build trees offline**: Don't build on-demand\n",
    "2. **Cache tree structure**: Save to disk/database\n",
    "3. **Update incrementally**: Add new docs without full rebuild\n",
    "4. **Tune clustering**: Experiment with cluster sizes\n",
    "5. **Hybrid retrieval**: Combine RAPTOR with flat search\n",
    "6. **Monitor tree quality**: Check summary coherence\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Part 8**, we'll implement **ColBERT**:\n",
    "- Token-level embeddings\n",
    "- Late interaction retrieval\n",
    "- MaxSim scoring\n",
    "- Better for technical content and code\n",
    "- Precise matching for security patterns\n",
    "\n",
    "Example: Finding similar code vulnerability patterns, matching exploit signatures.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Practice Exercises\n",
    "\n",
    "1. **Implement Tree Persistence**: Save/load RAPTOR trees\n",
    "2. **Add Metadata Propagation**: Preserve metadata up the tree\n",
    "3. **Implement Tree Traversal**: Navigate parent → child\n",
    "4. **Optimize Clustering**: Try different algorithms (HDBSCAN, Agglomerative)\n",
    "5. **Build MITRE ATT&CK Tree**: Organize by tactics/techniques\n",
    "\n",
    "### 📚 Further Reading\n",
    "\n",
    "- [RAPTOR Paper](https://arxiv.org/abs/2401.18059)\n",
    "- [Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n",
    "- [Document Summarization](https://arxiv.org/abs/2304.07129)\n",
    "- [Tree-Based Retrieval](https://arxiv.org/abs/2212.14024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
