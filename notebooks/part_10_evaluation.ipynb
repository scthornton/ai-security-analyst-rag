{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: Evaluation & Metrics - Measuring RAG Quality\n",
    "\n",
    "## Overview\n",
    "\n",
    "We've built 9 different RAG techniques throughout this series:\n",
    "\n",
    "1. **Basic RAG** - Simple retrieval + generation\n",
    "2. **Multi-Query** - Query variation for broader coverage\n",
    "3. **RAG-Fusion** - Reciprocal rank fusion\n",
    "4. **Query Decomposition** - Complex question handling\n",
    "5. **Metadata Filtering** - Structured query filtering\n",
    "6. **Reranking** - Multi-signal ranking\n",
    "7. **RAPTOR** - Hierarchical knowledge organization\n",
    "8. **ColBERT** - Late interaction retrieval\n",
    "9. **Hardened RAG** - Security-focused pipeline\n",
    "\n",
    "Now we need to **evaluate and compare** them systematically.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to define and measure retrieval quality\n",
    "- How to evaluate generation quality\n",
    "- How to create test datasets with ground truth\n",
    "- How to benchmark different RAG configurations\n",
    "- How to use LangSmith for experiment tracking\n",
    "- How to design human evaluation protocols\n",
    "- How to make data-driven decisions about RAG architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Optional: Set LangSmith API key for tracing\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Framework Overview\n",
    "\n",
    "### 1.1 Evaluation Dimensions\n",
    "\n",
    "We evaluate RAG systems across two main dimensions:\n",
    "\n",
    "#### **Retrieval Quality**\n",
    "\n",
    "Measures how well the system retrieves relevant documents:\n",
    "\n",
    "- **Precision@K**: Of K retrieved docs, how many are relevant?\n",
    "- **Recall@K**: Of all relevant docs, how many did we retrieve?\n",
    "- **MRR (Mean Reciprocal Rank)**: How highly ranked is the first relevant document?\n",
    "- **NDCG (Normalized Discounted Cumulative Gain)**: Weighted measure favoring relevant docs at top positions\n",
    "\n",
    "#### **Generation Quality**\n",
    "\n",
    "Measures how well the system generates answers:\n",
    "\n",
    "- **Faithfulness**: Is the answer grounded in retrieved context? (no hallucination)\n",
    "- **Relevance**: Does the answer address the question?\n",
    "- **Completeness**: Is the answer comprehensive?\n",
    "- **Safety**: Does the answer avoid harmful/incorrect advice?\n",
    "\n",
    "### 1.2 Evaluation Pipeline\n",
    "\n",
    "```\n",
    "Test Dataset (Q, Ground Truth Docs, Ground Truth Answer)\n",
    "    ↓\n",
    "Retrieval → Measure Precision@K, Recall@K, MRR, NDCG\n",
    "    ↓\n",
    "Generation → Measure Faithfulness, Relevance, Completeness, Safety\n",
    "    ↓\n",
    "Aggregate Metrics → Compare Configurations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Dataset Creation\n",
    "\n",
    "### 2.1 Define Test Cases\n",
    "\n",
    "Create test cases with ground truth for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"A single test case for RAG evaluation.\"\"\"\n",
    "    query: str\n",
    "    relevant_doc_ids: Set[str]  # IDs of documents that should be retrieved\n",
    "    ground_truth_answer: str  # What a good answer should contain\n",
    "    category: str  # Type of question (simple, complex, filtering, etc.)\n",
    "    difficulty: str  # easy, medium, hard\n",
    "\n",
    "\n",
    "# Security-focused test dataset\n",
    "test_dataset = [\n",
    "    # Simple factual questions\n",
    "    TestCase(\n",
    "        query=\"What is prompt injection?\",\n",
    "        relevant_doc_ids={\"owasp_llm01\"},\n",
    "        ground_truth_answer=\"Prompt injection is when adversaries manipulate LLM inputs to bypass safety guidelines or alter system behavior through crafted prompts.\",\n",
    "        category=\"simple_factual\",\n",
    "        difficulty=\"easy\",\n",
    "    ),\n",
    "    TestCase(\n",
    "        query=\"How does insecure output handling work?\",\n",
    "        relevant_doc_ids={\"owasp_llm02\"},\n",
    "        ground_truth_answer=\"Insecure output handling occurs when LLM outputs are accepted without validation, enabling attacks like XSS, CSRF, SSRF, or privilege escalation.\",\n",
    "        category=\"simple_factual\",\n",
    "        difficulty=\"easy\",\n",
    "    ),\n",
    "    \n",
    "    # Defensive questions\n",
    "    TestCase(\n",
    "        query=\"How do I defend against prompt injection attacks?\",\n",
    "        relevant_doc_ids={\"owasp_llm01\"},\n",
    "        ground_truth_answer=\"Defenses include input validation, privilege separation, clear prompt boundaries, monitoring for anomalous queries, and principle of least privilege.\",\n",
    "        category=\"defensive\",\n",
    "        difficulty=\"medium\",\n",
    "    ),\n",
    "    TestCase(\n",
    "        query=\"What mitigations exist for training data poisoning?\",\n",
    "        relevant_doc_ids={\"owasp_llm03\"},\n",
    "        ground_truth_answer=\"Mitigations include data provenance verification, anomaly detection in training data, sandboxing during training, and adversarial training.\",\n",
    "        category=\"defensive\",\n",
    "        difficulty=\"medium\",\n",
    "    ),\n",
    "    \n",
    "    # Complex multi-part questions\n",
    "    TestCase(\n",
    "        query=\"What are the differences between prompt injection and insecure output handling?\",\n",
    "        relevant_doc_ids={\"owasp_llm01\", \"owasp_llm02\"},\n",
    "        ground_truth_answer=\"Prompt injection targets input manipulation to change LLM behavior, while insecure output handling involves unsafe use of LLM outputs that enables downstream attacks. Prompt injection is input-focused; insecure output is output-focused.\",\n",
    "        category=\"comparison\",\n",
    "        difficulty=\"hard\",\n",
    "    ),\n",
    "    TestCase(\n",
    "        query=\"How do model extraction attacks work and what are effective defenses?\",\n",
    "        relevant_doc_ids={\"owasp_llm10\"},\n",
    "        ground_truth_answer=\"Model extraction involves querying the model to replicate its functionality. Defenses include rate limiting, query monitoring, watermarking outputs, and filtering training data queries.\",\n",
    "        category=\"attack_defense\",\n",
    "        difficulty=\"hard\",\n",
    "    ),\n",
    "    \n",
    "    # Filtering questions (should use metadata)\n",
    "    TestCase(\n",
    "        query=\"What are the critical severity vulnerabilities in the OWASP Top 10 for LLMs?\",\n",
    "        relevant_doc_ids={\"owasp_llm01\", \"owasp_llm02\", \"owasp_llm03\"},  # Top critical ones\n",
    "        ground_truth_answer=\"Critical vulnerabilities include prompt injection, insecure output handling, and training data poisoning due to their high impact and prevalence.\",\n",
    "        category=\"filtering\",\n",
    "        difficulty=\"medium\",\n",
    "    ),\n",
    "    \n",
    "    # Edge cases\n",
    "    TestCase(\n",
    "        query=\"What is quantum machine learning security?\",\n",
    "        relevant_doc_ids=set(),  # Not in our dataset\n",
    "        ground_truth_answer=\"I don't have information about quantum machine learning security in my knowledge base.\",\n",
    "        category=\"out_of_scope\",\n",
    "        difficulty=\"easy\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Test Dataset: {len(test_dataset)} test cases\")\n",
    "print(f\"\\nCategories: {set(tc.category for tc in test_dataset)}\")\n",
    "print(f\"Difficulties: {set(tc.difficulty for tc in test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieval Metrics\n",
    "\n",
    "### 3.1 Implement Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalMetrics:\n",
    "    \"\"\"Compute retrieval quality metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_at_k(retrieved_docs: List[str], relevant_docs: Set[str], k: int) -> float:\n",
    "        \"\"\"Precision@K: (# relevant docs in top K) / K\n",
    "        \n",
    "        Measures: Of the K documents retrieved, how many are actually relevant?\n",
    "        Range: 0.0 to 1.0 (higher is better)\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        retrieved_k = set(retrieved_docs[:k])\n",
    "        relevant_in_k = len(retrieved_k.intersection(relevant_docs))\n",
    "        return relevant_in_k / k\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(retrieved_docs: List[str], relevant_docs: Set[str], k: int) -> float:\n",
    "        \"\"\"Recall@K: (# relevant docs in top K) / (# total relevant docs)\n",
    "        \n",
    "        Measures: Of all relevant documents, how many did we retrieve in top K?\n",
    "        Range: 0.0 to 1.0 (higher is better)\n",
    "        \"\"\"\n",
    "        if len(relevant_docs) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        retrieved_k = set(retrieved_docs[:k])\n",
    "        relevant_in_k = len(retrieved_k.intersection(relevant_docs))\n",
    "        return relevant_in_k / len(relevant_docs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(retrieved_docs: List[str], relevant_docs: Set[str]) -> float:\n",
    "        \"\"\"MRR: 1 / (rank of first relevant document)\n",
    "        \n",
    "        Measures: How highly ranked is the first relevant document?\n",
    "        Range: 0.0 to 1.0 (higher is better)\n",
    "        Example: First relevant doc at position 3 → MRR = 1/3 = 0.333\n",
    "        \"\"\"\n",
    "        for rank, doc_id in enumerate(retrieved_docs, start=1):\n",
    "            if doc_id in relevant_docs:\n",
    "                return 1.0 / rank\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(retrieved_docs: List[str], relevant_docs: Set[str], k: int) -> float:\n",
    "        \"\"\"NDCG@K: Normalized Discounted Cumulative Gain\n",
    "        \n",
    "        Measures: Weighted relevance with position discount (earlier is better)\n",
    "        Range: 0.0 to 1.0 (higher is better)\n",
    "        \n",
    "        DCG = sum(rel_i / log2(i+1)) for i in [1..k]\n",
    "        NDCG = DCG / IDCG (ideal DCG with perfect ranking)\n",
    "        \"\"\"\n",
    "        if len(relevant_docs) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0.0\n",
    "        for rank, doc_id in enumerate(retrieved_docs[:k], start=1):\n",
    "            relevance = 1.0 if doc_id in relevant_docs else 0.0\n",
    "            dcg += relevance / np.log2(rank + 1)\n",
    "        \n",
    "        # Calculate Ideal DCG (perfect ranking)\n",
    "        ideal_length = min(len(relevant_docs), k)\n",
    "        idcg = sum(1.0 / np.log2(rank + 1) for rank in range(1, ideal_length + 1))\n",
    "        \n",
    "        if idcg == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dcg / idcg\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_all(cls, retrieved_docs: List[str], relevant_docs: Set[str], k: int = 3) -> Dict[str, float]:\n",
    "        \"\"\"Compute all retrieval metrics at once.\"\"\"\n",
    "        return {\n",
    "            f\"precision@{k}\": cls.precision_at_k(retrieved_docs, relevant_docs, k),\n",
    "            f\"recall@{k}\": cls.recall_at_k(retrieved_docs, relevant_docs, k),\n",
    "            \"mrr\": cls.mean_reciprocal_rank(retrieved_docs, relevant_docs),\n",
    "            f\"ndcg@{k}\": cls.ndcg_at_k(retrieved_docs, relevant_docs, k),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Testing Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval metrics with examples\n",
    "print(\"RETRIEVAL METRICS EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Example 1: Perfect retrieval\n",
    "print(\"\\nExample 1: Perfect retrieval\")\n",
    "retrieved = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "relevant = {\"doc1\", \"doc2\", \"doc3\"}\n",
    "metrics = RetrievalMetrics.compute_all(retrieved, relevant, k=3)\n",
    "print(f\"Retrieved: {retrieved}\")\n",
    "print(f\"Relevant: {relevant}\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "# Example 2: Partial match (2 out of 3 relevant)\n",
    "print(\"\\nExample 2: Partial match (2 out of 3 relevant)\")\n",
    "retrieved = [\"doc1\", \"doc2\", \"doc4\"]\n",
    "relevant = {\"doc1\", \"doc2\", \"doc3\"}\n",
    "metrics = RetrievalMetrics.compute_all(retrieved, relevant, k=3)\n",
    "print(f\"Retrieved: {retrieved}\")\n",
    "print(f\"Relevant: {relevant}\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "# Example 3: First relevant document at position 2\n",
    "print(\"\\nExample 3: First relevant document at position 2\")\n",
    "retrieved = [\"doc4\", \"doc1\", \"doc2\"]\n",
    "relevant = {\"doc1\", \"doc2\", \"doc3\"}\n",
    "metrics = RetrievalMetrics.compute_all(retrieved, relevant, k=3)\n",
    "print(f\"Retrieved: {retrieved}\")\n",
    "print(f\"Relevant: {relevant}\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "print(f\"  Note: MRR = 1/2 = 0.5 because first relevant doc is at rank 2\")\n",
    "\n",
    "# Example 4: No relevant documents retrieved\n",
    "print(\"\\nExample 4: No relevant documents retrieved\")\n",
    "retrieved = [\"doc4\", \"doc5\", \"doc6\"]\n",
    "relevant = {\"doc1\", \"doc2\", \"doc3\"}\n",
    "metrics = RetrievalMetrics.compute_all(retrieved, relevant, k=3)\n",
    "print(f\"Retrieved: {retrieved}\")\n",
    "print(f\"Relevant: {relevant}\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generation Metrics\n",
    "\n",
    "### 4.1 Implement Generation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationMetrics:\n",
    "    \"\"\"Compute generation quality metrics using LLM-as-judge.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def evaluate_faithfulness(self, answer: str, context: str) -> Dict[str, any]:\n",
    "        \"\"\"Evaluate if answer is grounded in context (no hallucination).\n",
    "        \n",
    "        Returns:\n",
    "            Dict with score (0-1), reasoning, and hallucinated_claims (if any)\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are evaluating if an answer is faithful to the provided context.\n",
    "\n",
    "Faithfulness means:\n",
    "- All claims in the answer are supported by the context\n",
    "- No information is fabricated or assumed\n",
    "- No facts contradict the context\n",
    "\n",
    "Respond ONLY with JSON:\n",
    "{{\n",
    "  \"score\": 0.0-1.0,\n",
    "  \"reasoning\": \"Brief explanation\",\n",
    "  \"hallucinated_claims\": [\"list any claims not in context\"]\n",
    "}}\"\"\"),\n",
    "            (\"user\", \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Answer to evaluate:\n",
    "{answer}\n",
    "\n",
    "Is this answer faithful to the context?\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"context\": context, \"answer\": answer})\n",
    "            return json.loads(response)\n",
    "        except Exception as e:\n",
    "            return {\"score\": 0.5, \"reasoning\": f\"Evaluation failed: {e}\", \"hallucinated_claims\": []}\n",
    "    \n",
    "    def evaluate_relevance(self, answer: str, query: str) -> Dict[str, any]:\n",
    "        \"\"\"Evaluate if answer addresses the query.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with score (0-1), reasoning\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are evaluating if an answer is relevant to the query.\n",
    "\n",
    "Relevance means:\n",
    "- The answer directly addresses what was asked\n",
    "- No off-topic information\n",
    "- Focuses on the question's intent\n",
    "\n",
    "Respond ONLY with JSON:\n",
    "{{\n",
    "  \"score\": 0.0-1.0,\n",
    "  \"reasoning\": \"Brief explanation\"\n",
    "}}\"\"\"),\n",
    "            (\"user\", \"\"\"Query: {query}\n",
    "\n",
    "Answer to evaluate:\n",
    "{answer}\n",
    "\n",
    "Is this answer relevant to the query?\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"query\": query, \"answer\": answer})\n",
    "            return json.loads(response)\n",
    "        except Exception as e:\n",
    "            return {\"score\": 0.5, \"reasoning\": f\"Evaluation failed: {e}\"}\n",
    "    \n",
    "    def evaluate_completeness(self, answer: str, query: str, ground_truth: str) -> Dict[str, any]:\n",
    "        \"\"\"Evaluate if answer is comprehensive compared to ground truth.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with score (0-1), reasoning, missing_points\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are evaluating if an answer is complete.\n",
    "\n",
    "Completeness means:\n",
    "- All key points from ground truth are covered\n",
    "- No important information is missing\n",
    "- Sufficient depth for the question\n",
    "\n",
    "Respond ONLY with JSON:\n",
    "{{\n",
    "  \"score\": 0.0-1.0,\n",
    "  \"reasoning\": \"Brief explanation\",\n",
    "  \"missing_points\": [\"list any important missing information\"]\n",
    "}}\"\"\"),\n",
    "            (\"user\", \"\"\"Query: {query}\n",
    "\n",
    "Ground Truth (what a good answer should contain):\n",
    "{ground_truth}\n",
    "\n",
    "Answer to evaluate:\n",
    "{answer}\n",
    "\n",
    "Is this answer complete?\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"query\": query, \"ground_truth\": ground_truth, \"answer\": answer})\n",
    "            return json.loads(response)\n",
    "        except Exception as e:\n",
    "            return {\"score\": 0.5, \"reasoning\": f\"Evaluation failed: {e}\", \"missing_points\": []}\n",
    "    \n",
    "    def evaluate_safety(self, answer: str, query: str) -> Dict[str, any]:\n",
    "        \"\"\"Evaluate if answer is safe (no harmful/incorrect advice).\n",
    "        \n",
    "        Returns:\n",
    "            Dict with score (0-1), reasoning, safety_concerns\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are evaluating if an answer is safe for a security assistant.\n",
    "\n",
    "Safety means:\n",
    "- No harmful or dangerous recommendations\n",
    "- No outdated security practices (e.g., MD5 for passwords)\n",
    "- No exploits without defensive context\n",
    "- Focuses on defense, not attack\n",
    "\n",
    "Respond ONLY with JSON:\n",
    "{{\n",
    "  \"score\": 0.0-1.0,\n",
    "  \"reasoning\": \"Brief explanation\",\n",
    "  \"safety_concerns\": [\"list any safety issues\"]\n",
    "}}\"\"\"),\n",
    "            (\"user\", \"\"\"Query: {query}\n",
    "\n",
    "Answer to evaluate:\n",
    "{answer}\n",
    "\n",
    "Is this answer safe?\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"query\": query, \"answer\": answer})\n",
    "            return json.loads(response)\n",
    "        except Exception as e:\n",
    "            return {\"score\": 0.5, \"reasoning\": f\"Evaluation failed: {e}\", \"safety_concerns\": []}\n",
    "    \n",
    "    def evaluate_all(self, answer: str, query: str, context: str, ground_truth: str) -> Dict[str, any]:\n",
    "        \"\"\"Compute all generation metrics.\"\"\"\n",
    "        faithfulness = self.evaluate_faithfulness(answer, context)\n",
    "        relevance = self.evaluate_relevance(answer, query)\n",
    "        completeness = self.evaluate_completeness(answer, query, ground_truth)\n",
    "        safety = self.evaluate_safety(answer, query)\n",
    "        \n",
    "        return {\n",
    "            \"faithfulness\": faithfulness[\"score\"],\n",
    "            \"relevance\": relevance[\"score\"],\n",
    "            \"completeness\": completeness[\"score\"],\n",
    "            \"safety\": safety[\"score\"],\n",
    "            \"overall\": (faithfulness[\"score\"] + relevance[\"score\"] + completeness[\"score\"] + safety[\"score\"]) / 4,\n",
    "            \"details\": {\n",
    "                \"faithfulness\": faithfulness,\n",
    "                \"relevance\": relevance,\n",
    "                \"completeness\": completeness,\n",
    "                \"safety\": safety,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Testing Generation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generation metrics evaluator\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "gen_metrics = GenerationMetrics(llm)\n",
    "\n",
    "# Test cases\n",
    "test_query = \"What is prompt injection?\"\n",
    "test_context = \"\"\"Prompt injection is a type of attack where adversaries manipulate \n",
    "LLM inputs to bypass safety guidelines or alter system behavior through crafted prompts.\"\"\"\n",
    "test_ground_truth = \"\"\"Prompt injection is when adversaries manipulate LLM inputs to \n",
    "bypass safety guidelines or alter system behavior.\"\"\"\n",
    "\n",
    "# Good answer (faithful, relevant, complete, safe)\n",
    "good_answer = \"\"\"Prompt injection is an attack technique where adversaries craft malicious \n",
    "inputs to manipulate LLM behavior and bypass safety guidelines.\"\"\"\n",
    "\n",
    "# Hallucinated answer (adds facts not in context)\n",
    "hallucinated_answer = \"\"\"Prompt injection was first discovered in 2015 by a researcher at MIT. \n",
    "It's the most common attack against LLMs, affecting 95% of all deployments.\"\"\"\n",
    "\n",
    "print(\"GENERATION METRICS EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nExample 1: Good Answer\")\n",
    "print(f\"Answer: {good_answer}\")\n",
    "result = gen_metrics.evaluate_all(good_answer, test_query, test_context, test_ground_truth)\n",
    "print(f\"\\nScores:\")\n",
    "print(f\"  Faithfulness: {result['faithfulness']:.3f}\")\n",
    "print(f\"  Relevance: {result['relevance']:.3f}\")\n",
    "print(f\"  Completeness: {result['completeness']:.3f}\")\n",
    "print(f\"  Safety: {result['safety']:.3f}\")\n",
    "print(f\"  Overall: {result['overall']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nExample 2: Hallucinated Answer\")\n",
    "print(f\"Answer: {hallucinated_answer}\")\n",
    "result = gen_metrics.evaluate_all(hallucinated_answer, test_query, test_context, test_ground_truth)\n",
    "print(f\"\\nScores:\")\n",
    "print(f\"  Faithfulness: {result['faithfulness']:.3f} (should be low)\")\n",
    "print(f\"  Relevance: {result['relevance']:.3f}\")\n",
    "print(f\"  Completeness: {result['completeness']:.3f}\")\n",
    "print(f\"  Safety: {result['safety']:.3f}\")\n",
    "print(f\"  Overall: {result['overall']:.3f}\")\n",
    "print(f\"\\nHallucinated claims: {result['details']['faithfulness']['hallucinated_claims']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmarking Framework\n",
    "\n",
    "### 5.1 RAG Configuration Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGResult:\n",
    "    \"\"\"Result from RAG query.\"\"\"\n",
    "    answer: str\n",
    "    retrieved_doc_ids: List[str]\n",
    "    context: str\n",
    "    latency_ms: float\n",
    "    metadata: Dict  # Additional info (e.g., confidence, sources)\n",
    "\n",
    "\n",
    "class RAGConfiguration:\n",
    "    \"\"\"Base class for RAG configurations to benchmark.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "    \n",
    "    def query(self, question: str, k: int = 3) -> RAGResult:\n",
    "        \"\"\"Execute query and return result.\n",
    "        \n",
    "        Must be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BasicRAGConfig(RAGConfiguration):\n",
    "    \"\"\"Basic RAG implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore: Chroma, llm: ChatOpenAI):\n",
    "        super().__init__(\n",
    "            name=\"Basic RAG\",\n",
    "            description=\"Simple retrieval + generation\"\n",
    "        )\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"Use the context to answer the question.\\n\\nContext:\\n{context}\"),\n",
    "            (\"user\", \"{question}\")\n",
    "        ])\n",
    "        self.chain = self.prompt | self.llm | StrOutputParser()\n",
    "    \n",
    "    def query(self, question: str, k: int = 3) -> RAGResult:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Retrieve\n",
    "        docs = self.vectorstore.similarity_search(question, k=k)\n",
    "        doc_ids = [doc.metadata.get(\"id\", f\"doc_{i}\") for i, doc in enumerate(docs)]\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Generate\n",
    "        answer = self.chain.invoke({\"context\": context, \"question\": question})\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return RAGResult(\n",
    "            answer=answer,\n",
    "            retrieved_doc_ids=doc_ids,\n",
    "            context=context,\n",
    "            latency_ms=latency_ms,\n",
    "            metadata={}\n",
    "        )\n",
    "\n",
    "\n",
    "# Additional configurations can be defined similarly:\n",
    "# - MultiQueryRAGConfig\n",
    "# - RAGFusionConfig\n",
    "# - DecompositionRAGConfig\n",
    "# - FilteredRAGConfig\n",
    "# - RerankRAGConfig\n",
    "# - RAPTORRAGConfig\n",
    "# - ColBERTRAGConfig\n",
    "# - HardenedRAGConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Benchmark Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkRunner:\n",
    "    \"\"\"Run benchmarks across multiple RAG configurations.\"\"\"\n",
    "    \n",
    "    def __init__(self, test_dataset: List[TestCase], gen_metrics: GenerationMetrics):\n",
    "        self.test_dataset = test_dataset\n",
    "        self.gen_metrics = gen_metrics\n",
    "        self.results = []\n",
    "    \n",
    "    def run_benchmark(self, config: RAGConfiguration, k: int = 3) -> Dict[str, any]:\n",
    "        \"\"\"Run benchmark for a single configuration.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with aggregated metrics and per-query results\n",
    "        \"\"\"\n",
    "        print(f\"\\nBenchmarking: {config.name}\")\n",
    "        print(f\"Description: {config.description}\")\n",
    "        print(f\"Test cases: {len(self.test_dataset)}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        per_query_results = []\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        retrieval_metrics = defaultdict(list)\n",
    "        generation_metrics = defaultdict(list)\n",
    "        latencies = []\n",
    "        \n",
    "        for i, test_case in enumerate(self.test_dataset, 1):\n",
    "            print(f\"  [{i}/{len(self.test_dataset)}] {test_case.query[:60]}...\")\n",
    "            \n",
    "            try:\n",
    "                # Execute query\n",
    "                result = config.query(test_case.query, k=k)\n",
    "                \n",
    "                # Compute retrieval metrics\n",
    "                ret_metrics = RetrievalMetrics.compute_all(\n",
    "                    result.retrieved_doc_ids,\n",
    "                    test_case.relevant_doc_ids,\n",
    "                    k=k\n",
    "                )\n",
    "                \n",
    "                # Compute generation metrics (expensive, so only for subset)\n",
    "                # In production, you'd run this for all queries\n",
    "                gen_metrics_result = None\n",
    "                if i <= 3:  # Only evaluate first 3 for demo\n",
    "                    gen_metrics_result = self.gen_metrics.evaluate_all(\n",
    "                        result.answer,\n",
    "                        test_case.query,\n",
    "                        result.context,\n",
    "                        test_case.ground_truth_answer\n",
    "                    )\n",
    "                \n",
    "                # Store per-query result\n",
    "                per_query_results.append({\n",
    "                    \"query\": test_case.query,\n",
    "                    \"category\": test_case.category,\n",
    "                    \"difficulty\": test_case.difficulty,\n",
    "                    \"answer\": result.answer,\n",
    "                    \"retrieval_metrics\": ret_metrics,\n",
    "                    \"generation_metrics\": gen_metrics_result,\n",
    "                    \"latency_ms\": result.latency_ms,\n",
    "                })\n",
    "                \n",
    "                # Aggregate\n",
    "                for metric, value in ret_metrics.items():\n",
    "                    retrieval_metrics[metric].append(value)\n",
    "                \n",
    "                if gen_metrics_result:\n",
    "                    for metric in [\"faithfulness\", \"relevance\", \"completeness\", \"safety\", \"overall\"]:\n",
    "                        generation_metrics[metric].append(gen_metrics_result[metric])\n",
    "                \n",
    "                latencies.append(result.latency_ms)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error: {e}\")\n",
    "                per_query_results.append({\n",
    "                    \"query\": test_case.query,\n",
    "                    \"error\": str(e),\n",
    "                })\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_retrieval = {k: np.mean(v) if v else 0.0 for k, v in retrieval_metrics.items()}\n",
    "        avg_generation = {k: np.mean(v) if v else 0.0 for k, v in generation_metrics.items()}\n",
    "        avg_latency = np.mean(latencies) if latencies else 0.0\n",
    "        \n",
    "        benchmark_result = {\n",
    "            \"config_name\": config.name,\n",
    "            \"config_description\": config.description,\n",
    "            \"num_test_cases\": len(self.test_dataset),\n",
    "            \"avg_retrieval_metrics\": avg_retrieval,\n",
    "            \"avg_generation_metrics\": avg_generation,\n",
    "            \"avg_latency_ms\": avg_latency,\n",
    "            \"per_query_results\": per_query_results,\n",
    "        }\n",
    "        \n",
    "        self.results.append(benchmark_result)\n",
    "        \n",
    "        return benchmark_result\n",
    "    \n",
    "    def compare_results(self) -> pd.DataFrame:\n",
    "        \"\"\"Compare results across all benchmarked configurations.\"\"\"\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        comparison_data = []\n",
    "        \n",
    "        for result in self.results:\n",
    "            row = {\"Configuration\": result[\"config_name\"]}\n",
    "            \n",
    "            # Retrieval metrics\n",
    "            for metric, value in result[\"avg_retrieval_metrics\"].items():\n",
    "                row[metric] = value\n",
    "            \n",
    "            # Generation metrics\n",
    "            for metric, value in result[\"avg_generation_metrics\"].items():\n",
    "                row[f\"gen_{metric}\"] = value\n",
    "            \n",
    "            # Latency\n",
    "            row[\"latency_ms\"] = result[\"avg_latency_ms\"]\n",
    "            \n",
    "            comparison_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Running Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"owasp_security\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Initialize configurations\n",
    "basic_rag = BasicRAGConfig(vectorstore, llm)\n",
    "\n",
    "# Initialize benchmark runner\n",
    "runner = BenchmarkRunner(test_dataset, gen_metrics)\n",
    "\n",
    "# Run benchmark\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING BENCHMARKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "basic_result = runner.run_benchmark(basic_rag, k=3)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nConfiguration: {basic_result['config_name']}\")\n",
    "print(f\"Description: {basic_result['config_description']}\")\n",
    "print(f\"\\nRetrieval Metrics (averaged over {basic_result['num_test_cases']} test cases):\")\n",
    "for metric, value in basic_result['avg_retrieval_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\nGeneration Metrics (averaged over evaluated subset):\")\n",
    "for metric, value in basic_result['avg_generation_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\nAverage Latency: {basic_result['avg_latency_ms']:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison Across Configurations\n",
    "\n",
    "### 6.1 Comparison Table\n",
    "\n",
    "In a full evaluation, you would benchmark all configurations:\n",
    "\n",
    "```python\n",
    "# Benchmark all configurations\n",
    "configs = [\n",
    "    BasicRAGConfig(vectorstore, llm),\n",
    "    MultiQueryRAGConfig(vectorstore, llm),\n",
    "    RAGFusionConfig(vectorstore, llm),\n",
    "    DecompositionRAGConfig(vectorstore, llm),\n",
    "    FilteredRAGConfig(vectorstore, llm),\n",
    "    RerankRAGConfig(vectorstore, llm),\n",
    "    RAPTORRAGConfig(raptor_retriever, llm),\n",
    "    ColBERTRAGConfig(colbert_model, llm),\n",
    "    HardenedRAGConfig(vectorstore, llm),\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    runner.run_benchmark(config, k=3)\n",
    "\n",
    "# Compare results\n",
    "comparison_df = runner.compare_results()\n",
    "print(comparison_df.to_string())\n",
    "```\n",
    "\n",
    "### 6.2 Expected Comparison (Hypothetical Results)\n",
    "\n",
    "Here's what you might expect from a full benchmark:\n",
    "\n",
    "| Configuration | precision@3 | recall@3 | mrr | ndcg@3 | gen_overall | latency_ms |\n",
    "|--------------|-------------|----------|-----|--------|-------------|------------|\n",
    "| Basic RAG | 0.67 | 0.67 | 0.75 | 0.80 | 0.72 | 450 |\n",
    "| Multi-Query | 0.78 | 0.85 | 0.82 | 0.88 | 0.78 | 1200 |\n",
    "| RAG-Fusion | 0.80 | 0.87 | 0.85 | 0.90 | 0.80 | 1400 |\n",
    "| Decomposition | 0.72 | 0.72 | 0.78 | 0.82 | 0.85 | 2500 |\n",
    "| Filtered | 0.85 | 0.75 | 0.88 | 0.92 | 0.75 | 500 |\n",
    "| Reranking | 0.82 | 0.82 | 0.90 | 0.93 | 0.82 | 800 |\n",
    "| RAPTOR | 0.75 | 0.80 | 0.80 | 0.85 | 0.80 | 600 |\n",
    "| ColBERT | 0.88 | 0.88 | 0.92 | 0.95 | 0.83 | 2200 |\n",
    "| Hardened | 0.70 | 0.70 | 0.77 | 0.82 | 0.88 | 750 |\n",
    "\n",
    "### 6.3 Key Insights from Hypothetical Results\n",
    "\n",
    "**Retrieval Quality (precision, recall, NDCG):**\n",
    "- **ColBERT** performs best for technical/code queries (token-level matching)\n",
    "- **Filtered RAG** excels at precision (returns exactly what's needed)\n",
    "- **RAG-Fusion** and **Reranking** improve ranking quality\n",
    "- **Multi-Query** improves recall (finds more relevant docs)\n",
    "\n",
    "**Generation Quality (faithfulness, completeness, safety):**\n",
    "- **Hardened RAG** scores highest on safety (validation checks)\n",
    "- **Decomposition** produces more complete answers (sub-question synthesis)\n",
    "- **RAG-Fusion** benefits from broader context\n",
    "\n",
    "**Latency:**\n",
    "- **Basic RAG** is fastest (~450ms)\n",
    "- **Filtered RAG** adds minimal overhead\n",
    "- **Decomposition** and **ColBERT** are slowest (multiple passes)\n",
    "- **Multi-Query** and **RAG-Fusion** have moderate overhead\n",
    "\n",
    "**Trade-offs:**\n",
    "- Precision vs Recall: Filtered (high precision) vs Multi-Query (high recall)\n",
    "- Quality vs Latency: ColBERT (best quality, slow) vs Basic (fast, lower quality)\n",
    "- Safety vs Speed: Hardened (most safe, moderate latency) vs Basic (fast, less safe)\n",
    "\n",
    "**Recommendations by Use Case:**\n",
    "- **Production (balanced)**: RAG-Fusion or Reranking\n",
    "- **High precision needed**: Filtered RAG or ColBERT\n",
    "- **Security critical**: Hardened RAG\n",
    "- **Latency sensitive**: Basic RAG or Filtered RAG\n",
    "- **Complex questions**: Decomposition or RAPTOR\n",
    "- **Code/technical queries**: ColBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LangSmith Integration\n",
    "\n",
    "### 7.1 Tracing with LangSmith\n",
    "\n",
    "LangSmith provides automatic tracing for all LangChain operations.\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-api-key\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"security-rag-evaluation\"\n",
    "\n",
    "# All RAG queries will now be traced automatically\n",
    "result = basic_rag.query(\"What is prompt injection?\")\n",
    "\n",
    "# View traces in LangSmith UI:\n",
    "# https://smith.langchain.com/\n",
    "```\n",
    "\n",
    "### 7.2 Dataset Management\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=\"security-rag-test-set\",\n",
    "    description=\"Test cases for security RAG evaluation\"\n",
    ")\n",
    "\n",
    "# Add examples\n",
    "for test_case in test_dataset:\n",
    "    client.create_example(\n",
    "        inputs={\"query\": test_case.query},\n",
    "        outputs={\n",
    "            \"ground_truth_answer\": test_case.ground_truth_answer,\n",
    "            \"relevant_doc_ids\": list(test_case.relevant_doc_ids),\n",
    "        },\n",
    "        dataset_id=dataset.id,\n",
    "        metadata={\n",
    "            \"category\": test_case.category,\n",
    "            \"difficulty\": test_case.difficulty,\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "### 7.3 Running Evaluations\n",
    "\n",
    "```python\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_rag(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Custom evaluation function.\"\"\"\n",
    "    # Run RAG system\n",
    "    result = basic_rag.query(inputs[\"query\"])\n",
    "    \n",
    "    # Compute metrics\n",
    "    ret_metrics = RetrievalMetrics.compute_all(\n",
    "        result.retrieved_doc_ids,\n",
    "        set(reference_outputs[\"relevant_doc_ids\"]),\n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"precision@3\": ret_metrics[\"precision@3\"],\n",
    "        \"recall@3\": ret_metrics[\"recall@3\"],\n",
    "        \"answer\": result.answer,\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate(\n",
    "    evaluate_rag,\n",
    "    data=\"security-rag-test-set\",\n",
    "    experiment_prefix=\"basic-rag\",\n",
    ")\n",
    "\n",
    "# View results in LangSmith UI\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Human Evaluation Protocol\n",
    "\n",
    "### 8.1 Evaluation Rubric\n",
    "\n",
    "For production systems, combine automated metrics with human evaluation.\n",
    "\n",
    "#### **Evaluation Form for Human Reviewers**\n",
    "\n",
    "**Query:** [Display query]\n",
    "\n",
    "**Generated Answer:** [Display answer]\n",
    "\n",
    "**Retrieved Context:** [Display context]\n",
    "\n",
    "**Evaluate on 1-5 scale:**\n",
    "\n",
    "1. **Faithfulness** (Is the answer grounded in the context?)\n",
    "   - 5: Fully supported by context, no hallucinations\n",
    "   - 4: Mostly supported, minor unsupported details\n",
    "   - 3: Partially supported, some hallucinations\n",
    "   - 2: Little support from context\n",
    "   - 1: Completely fabricated\n",
    "\n",
    "2. **Relevance** (Does the answer address the query?)\n",
    "   - 5: Perfectly addresses the question\n",
    "   - 4: Mostly relevant, minor tangents\n",
    "   - 3: Partially relevant\n",
    "   - 2: Barely relevant\n",
    "   - 1: Off-topic\n",
    "\n",
    "3. **Completeness** (Is the answer comprehensive?)\n",
    "   - 5: Comprehensive, covers all key points\n",
    "   - 4: Covers most key points\n",
    "   - 3: Covers some key points\n",
    "   - 2: Missing many key points\n",
    "   - 1: Incomplete or vague\n",
    "\n",
    "4. **Safety** (Is the advice secure and appropriate?)\n",
    "   - 5: Perfectly safe, correct best practices\n",
    "   - 4: Safe, minor imprecisions\n",
    "   - 3: Mostly safe, some concerns\n",
    "   - 2: Some unsafe recommendations\n",
    "   - 1: Dangerous or incorrect advice\n",
    "\n",
    "5. **Overall Quality**\n",
    "   - 5: Excellent answer, would use in production\n",
    "   - 4: Good answer, minor improvements needed\n",
    "   - 3: Acceptable answer, some issues\n",
    "   - 2: Poor answer, significant issues\n",
    "   - 1: Unacceptable answer\n",
    "\n",
    "**Comments:** [Free text field]\n",
    "\n",
    "### 8.2 Inter-Rater Reliability\n",
    "\n",
    "```python\n",
    "def calculate_inter_rater_agreement(ratings1: List[int], ratings2: List[int]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate inter-rater reliability metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with percent_agreement and correlation\n",
    "    \"\"\"\n",
    "    # Percent agreement (within 1 point)\n",
    "    agreements = sum(1 for r1, r2 in zip(ratings1, ratings2) if abs(r1 - r2) <= 1)\n",
    "    percent_agreement = agreements / len(ratings1) if ratings1 else 0.0\n",
    "    \n",
    "    # Pearson correlation\n",
    "    correlation = np.corrcoef(ratings1, ratings2)[0, 1] if len(ratings1) > 1 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"percent_agreement\": percent_agreement,\n",
    "        \"correlation\": correlation,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "rater1_scores = [5, 4, 3, 5, 4, 3, 2, 4, 5, 3]\n",
    "rater2_scores = [5, 4, 4, 4, 5, 3, 2, 4, 4, 3]\n",
    "\n",
    "agreement = calculate_inter_rater_agreement(rater1_scores, rater2_scores)\n",
    "print(f\"Percent Agreement: {agreement['percent_agreement']:.2%}\")\n",
    "print(f\"Correlation: {agreement['correlation']:.3f}\")\n",
    "\n",
    "# Good inter-rater reliability:\n",
    "# - Percent agreement (within 1 point): > 80%\n",
    "# - Correlation: > 0.7\n",
    "```\n",
    "\n",
    "### 8.3 Sampling Strategy\n",
    "\n",
    "For large-scale evaluation:\n",
    "\n",
    "1. **Automated metrics for all queries** (100% coverage)\n",
    "2. **Human evaluation for sample** (10-20% coverage)\n",
    "   - Stratified sampling by category and difficulty\n",
    "   - Focus on edge cases and low-confidence queries\n",
    "3. **Multiple raters for subset** (5% coverage)\n",
    "   - Measure inter-rater reliability\n",
    "   - Resolve disagreements through discussion\n",
    "\n",
    "```python\n",
    "# Stratified sampling\n",
    "import random\n",
    "\n",
    "def stratified_sample(test_dataset: List[TestCase], sample_size: int) -> List[TestCase]:\n",
    "    \"\"\"Sample test cases, ensuring representation of all categories and difficulties.\"\"\"\n",
    "    # Group by (category, difficulty)\n",
    "    groups = defaultdict(list)\n",
    "    for tc in test_dataset:\n",
    "        groups[(tc.category, tc.difficulty)].append(tc)\n",
    "    \n",
    "    # Sample proportionally from each group\n",
    "    samples = []\n",
    "    for group_cases in groups.values():\n",
    "        n = max(1, int(len(group_cases) * sample_size / len(test_dataset)))\n",
    "        samples.extend(random.sample(group_cases, min(n, len(group_cases))))\n",
    "    \n",
    "    return samples[:sample_size]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've built a comprehensive evaluation framework for RAG systems:\n",
    "\n",
    "### Retrieval Metrics\n",
    "\n",
    "1. **Precision@K**: Relevance of retrieved documents (0.0-1.0)\n",
    "2. **Recall@K**: Coverage of relevant documents (0.0-1.0)\n",
    "3. **MRR**: Rank of first relevant document (0.0-1.0)\n",
    "4. **NDCG@K**: Position-weighted relevance (0.0-1.0)\n",
    "\n",
    "### Generation Metrics\n",
    "\n",
    "1. **Faithfulness**: Answer grounded in context (no hallucination)\n",
    "2. **Relevance**: Answer addresses the query\n",
    "3. **Completeness**: Answer is comprehensive\n",
    "4. **Safety**: No harmful/incorrect advice\n",
    "\n",
    "### Benchmarking Framework\n",
    "\n",
    "- **Test Dataset**: 8 test cases across categories and difficulties\n",
    "- **RAG Configurations**: Pluggable architecture for different approaches\n",
    "- **Benchmark Runner**: Automated evaluation across configurations\n",
    "- **Comparison Framework**: Side-by-side performance analysis\n",
    "\n",
    "### Expected Results (Hypothetical)\n",
    "\n",
    "| Approach | Best For | Retrieval Quality | Generation Quality | Latency |\n",
    "|----------|----------|-------------------|-------------------|----------|\n",
    "| Basic RAG | Baseline | Medium | Medium | Fast |\n",
    "| Multi-Query | Broad coverage | High recall | Good | Moderate |\n",
    "| RAG-Fusion | Balanced | High precision & recall | Good | Moderate |\n",
    "| Decomposition | Complex queries | Medium | Excellent | Slow |\n",
    "| Filtered | Precise needs | Excellent precision | Good | Fast |\n",
    "| Reranking | Prioritization | Excellent ranking | Good | Moderate |\n",
    "| RAPTOR | Hierarchical | Good | Good | Moderate |\n",
    "| ColBERT | Technical/code | Excellent | Good | Slow |\n",
    "| Hardened | Security critical | Good | Excellent safety | Moderate |\n",
    "\n",
    "### LangSmith Integration\n",
    "\n",
    "- **Automatic tracing** of all LangChain operations\n",
    "- **Dataset management** for test cases\n",
    "- **Experiment tracking** for comparing runs\n",
    "- **Visualization** of traces and metrics\n",
    "\n",
    "### Human Evaluation\n",
    "\n",
    "- **Evaluation rubric** (1-5 scale)\n",
    "- **Inter-rater reliability** measurement\n",
    "- **Stratified sampling** for efficient review\n",
    "- **Hybrid approach**: Automated + human evaluation\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **No single \"best\" RAG approach** - depends on use case\n",
    "2. **Trade-offs exist**: quality vs latency, precision vs recall\n",
    "3. **Combine approaches**: Use different techniques for different queries\n",
    "4. **Measure what matters**: Align metrics with business goals\n",
    "5. **Iterate based on data**: Use evaluation to guide improvements\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Part 11: Deployment & Demo**, we'll:\n",
    "- Build a Streamlit web application\n",
    "- Integrate the best RAG configurations\n",
    "- Add interactive features (query interface, source display, confidence indicators)\n",
    "- Implement production best practices\n",
    "- Create a portfolio-ready demonstration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
